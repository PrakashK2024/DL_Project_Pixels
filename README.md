PixelSense uses a hybrid vision language pipeline that combines BLIP, BLIP-2, BLIP-Large, and InstructBLIP with a CLIP based reranking module. This setup gives us captions that are more accurate, more detailed, and more visually grounded than using any single model alone.
To extend the system beyond captioning, we added BLIP-VQA for question answering, YOLOv8 for object detection, and Pytesseract for OCR. These modules allow PixelSense to recognize objects, understand scenes, and extract text from images.
We fine tuned our models on a curated subset of the Flickr8k dataset using a mixed precision FP16 setup designed for limited compute. We also built a consistent evaluation framework using CLIP similarity, readability, and latency to measure gains in accuracy, fluency, and real time performance.
