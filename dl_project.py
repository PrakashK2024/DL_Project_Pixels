# -*- coding: utf-8 -*-
"""DL_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eo9_mFLyRmhBggVp1x6PCT7R3ZFFhv3L
"""

# Clone the repository
# Check if the directory exists before cloning
import os
if not os.path.exists('pycocoevalcap'):
    !git clone https://github.com/salaniz/pycocoevalcap.git
else:
    print("Directory 'pycocoevalcap' already exists. Skipping clone.")


# Navigate into the directory and install
# Add a check to ensure the directory exists before changing into it
if os.path.exists('pycocoevalcap'):
    !cd pycocoevalcap && python setup.py install
else:
    print("Directory 'pycocoevalcap' not found. Skipping installation.")


import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

import nltk
# Download NLTK data if not already present
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
try:
    nltk.data.find('corpora/omw-1.4')
except LookupError:
    nltk.download('omw-1.4')


# Install openjdk
!apt-get update
!apt-get install -y openjdk-11-jre-headless

import os

# Mount Google Drive to access Kaggle API key
from google.colab import drive
drive.mount('/content/drive')

# Create a directory for Kaggle credentials
# Check if the directory exists before creating
if not os.path.exists('~/.kaggle'):
    !mkdir ~/.kaggle'
else:
    print("Directory '~/.kaggle' already exists. Skipping mkdir.")


# Copy the Kaggle API key from Google Drive
# Check if the source file exists before copying
kaggle_json_path = '/content/drive/MyDrive/kaggle.json'
if os.path.exists(kaggle_json_path):
    !cp {kaggle_json_path} ~/.kaggle/
    # Set permissions for the Kaggle API key
    !chmod 600 ~/.kaggle/kaggle.json
    print("Kaggle API key copied and permissions set.")
else:
    print(f"Kaggle API key not found at {kaggle_json_path}. Skipping copy and chmod.")
    print("Please ensure your 'kaggle.json' file is in your Google Drive's root directory.")


# Download the dataset
# Check if the zip file already exists before downloading
dataset_zip_path = 'flickr8k.zip'
if not os.path.exists(dataset_zip_path):
    # Add a check for the Kaggle API key file before attempting to download
    if os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):
        !kaggle datasets download -d adityarajput1999/flickr8k
    else:
        print("Kaggle API key not found. Skipping dataset download.")
else:
    print(f"Dataset zip file '{dataset_zip_path}' already exists. Skipping download.")


# Unzip the dataset
# Check if the zip file exists before unzipping
dataset_extract_dir = '/content/archive'
if os.path.exists(dataset_zip_path):
    # Check if the destination directory already contains extracted files
    # This is a simple check; a more robust check might look for specific files/folders
    if not os.path.exists(dataset_extract_dir) or not os.listdir(dataset_extract_dir):
        !unzip {dataset_zip_path} -d {dataset_extract_dir}
        print(f"Dataset unzipped to {dataset_extract_dir}.")
    else:
        print(f"Dataset already seems to be extracted in {dataset_extract_dir}. Skipping unzip.")
else:
    print(f"Dataset zip file '{dataset_zip_path}' not found. Skipping unzip.")


# List the contents of the extracted directory
# Check if the directory exists before listing contents
if os.path.exists(dataset_extract_dir):
    print(f"\nContents of {dataset_extract_dir}:")
    !ls {dataset_extract_dir}
else:
    print(f"Extracted dataset directory '{dataset_extract_dir}' not found.")

!pip install -q gTTS
!apt install -q tesseract-ocr
!pip install -q pytesseract
!pip install -q ultralytics
!pip install -q textstat
!pip install -q transformers datasets accelerate torch torchvision
!pip install -q peft
!pip install -q numpy scipy nltk

from google.colab import drive
drive.mount('/content/drive')

# ==========================
# üì¶ Imports & Setup
# ==========================
from nltk.tokenize import word_tokenize
from PIL import Image
import torch
from google.colab import files
from huggingface_hub import logging as hf_logging
hf_logging.set_verbosity_error()
import logging
logging.getLogger("transformers").setLevel(logging.ERROR)
import warnings
warnings.filterwarnings("ignore")
from google.colab import files
from google.colab.output import eval_js
from IPython.display import display, Javascript, Image as IPImage, clear_output
from base64 import b64decode
from PIL import Image
import ipywidgets as widgets
import io
import time
import pandas as pd
from ultralytics import YOLO
import pytesseract
from pytesseract import Output
from PIL import ImageDraw
import torch
import nltk
from IPython.display import display, HTML
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from pycocoevalcap.cider.cider import Cider

from transformers import (
    BlipProcessor, BlipForConditionalGeneration,
    Blip2Processor, Blip2ForConditionalGeneration,
    CLIPProcessor, CLIPModel
)
from PIL import Image, ImageOps, ImageEnhance,ImageDraw, ImageFont
import matplotlib.pyplot as plt
from textstat import flesch_reading_ease
from IPython.display import display, HTML
from transformers import (
    BlipProcessor, BlipForConditionalGeneration,
    Blip2Processor, Blip2ForConditionalGeneration,
    Trainer, TrainingArguments
)
from PIL import Image
import torch, random, os
os.environ["HF_SAFE_TENSORS_MODE"] = "FALSE"
import os
os.environ["SPICE_JAR"] = "/usr/local/lib/python3.10/dist-packages/pycocoevalcap/spice/Spice.jar"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on device: {device}")

# --- Paths ---
folder_path = "/content/drive/MyDrive/archive/Images"
captions_path = "/content/drive/MyDrive/archive/captions.txt"

# --- Read captions.txt properly ---
data = []
with open(captions_path, "r") as f:
    for line in f:
        line = line.strip()
        if line:
            parts = line.split(',', 1)  # comma-separated
            if len(parts) == 2:
                image_id, caption = parts
                image_id = image_id.split('#')[0].strip()  # remove #0, #1, etc.
                caption = caption.strip()
                data.append([image_id, caption])

# Create DataFrame
df = pd.DataFrame(data, columns=["image_name", "caption_text"])

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
df = df.groupby("image_name")["caption_text"].apply(list).reset_index()
df["image_path"] = df["image_name"].apply(lambda x: os.path.join(folder_path, x))
df.head(2)

import torch
from torch.utils.data import DataLoader
from torch import nn, optim
from tqdm import tqdm
import random
from transformers import BlipProcessor, BlipForConditionalGeneration, Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import gc

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on device: {device}")

# ===============================
# --- Dataset class ---
# ===============================
class FlickrDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, processor):
        self.df = dataframe
        self.processor = processor

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image_path = row["image_path"]
        caption = random.choice(row["caption_text"])
        image = Image.open(image_path).convert("RGB")

        inputs = self.processor(
            images=image,
            text=caption,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        inputs["labels"] = inputs["input_ids"].clone()
        return {k: v.squeeze(0) for k, v in inputs.items()}

# ===============================
# --- Mixed precision fine-tuning function ---
# ===============================
def fine_tune_blip_amp(df, model_name, processor_class, model_class, output_dir,
                       epochs=1, batch_size=1, lr=5e-5, max_train_samples=200,
                       grad_accum_steps=8):

    print(f"\nüöÄ Fine-tuning {model_name} with mixed precision ...")

    # Processor & model
    processor = processor_class.from_pretrained(model_name)
    model = model_class.from_pretrained(model_name).to(device)
    model.train()

    # Dataset & DataLoader
    train_df = df.sample(max_train_samples, random_state=42).reset_index(drop=True)
    dataset = FlickrDataset(train_df, processor)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    optimizer = optim.AdamW(model.parameters(), lr=lr)
    scaler = torch.cuda.amp.GradScaler()  # for mixed precision

    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        epoch_loss = 0
        optimizer.zero_grad()

        for step, batch in enumerate(tqdm(loader)):
            inputs = {k: v.to(device) for k, v in batch.items()}

            with torch.cuda.amp.autocast():  # mixed precision
                outputs = model(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    pixel_values=inputs["pixel_values"],
                    labels=inputs["labels"]
                )
                loss = outputs.loss / grad_accum_steps  # normalize loss

            scaler.scale(loss).backward()

            if (step + 1) % grad_accum_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            epoch_loss += loss.item() * grad_accum_steps

        print(f"Epoch {epoch+1} loss: {epoch_loss/len(loader):.4f}")

    # Save model
    model.save_pretrained(output_dir, safe_serialization=False)
    processor.save_pretrained(output_dir)
    print(f"üíæ Saved fine-tuned model: {output_dir}")

    # Clean up GPU memory
    del model, processor, dataset, loader, optimizer, scaler
    gc.collect()
    torch.cuda.empty_cache()

# ----------- Load Pretrained Models (Base Only) -----------

print("üîÑ Loading pretrained models ...")

from transformers import (
    BlipProcessor, BlipForConditionalGeneration,
    Blip2Processor, Blip2ForConditionalGeneration,
    CLIPProcessor, CLIPModel
)
from ultralytics import YOLO
import torch

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# ==================== BLIP Models ====================

blip_proc_base = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model_base = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
).to(device)

blip_proc_large = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
blip_model_large = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-large"
).to(device)

# ==================== BLIP-2 Models ====================

blip2_proc_base = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
blip2_model_base = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-flan-t5-xl"
).to(device)

# ==================== InstructBLIP Models ====================

instruct_proc_base = Blip2Processor.from_pretrained("Salesforce/instructblip-flan-t5-xl")
instruct_model_base = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/instructblip-flan-t5-xl"
).to(device)

# ==================== CLIP + YOLO ====================

clip_proc = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)

yolo_model = YOLO("yolov8s.pt")

print("‚úÖ All base models loaded successfully!")

import re

# ----------- Utility: Clean up repetitive captions -----------
def clean_caption(text):
    """Removes repeated words, duplicate phrases, and extra spaces."""
    text = re.sub(r'\b(\w+)( \1\b)+', r'\1', text)  # remove consecutive duplicate words
    text = re.sub(r'\s{2,}', ' ', text)             # collapse multiple spaces
    text = text.replace(" : ", ": ")                # fix spacing after colons
    return text.strip().capitalize()


# ----------- BLIP captioning -----------
def caption_blip(img):
    proc = blip_proc_base
    model = blip_model_base


    inputs = proc(images=img, return_tensors="pt").to(device)

    out = model.generate(
        **inputs,
        do_sample=True,
        top_p=0.9,
        temperature=0.85,
        max_new_tokens=120,
        repetition_penalty=1.5,
        no_repeat_ngram_size=6,
        length_penalty=1.1,
        num_beams=5
    )

    caption = proc.decode(out[0], skip_special_tokens=True)
    return clean_caption(caption)


# ----------- BLIP-large captioning -----------
def caption_blip_large(img):
    proc = blip_proc_large
    model = blip_model_large

    inputs = proc(images=img, return_tensors="pt").to(device)

    out = model.generate(
        **inputs,
        do_sample=True,
        top_p=0.9,
        temperature=0.85,
        max_new_tokens=150,
        repetition_penalty=1.5,
        no_repeat_ngram_size=6,
        length_penalty=1.2,
        num_beams=6
    )

    caption = proc.decode(out[0], skip_special_tokens=True)
    return clean_caption(caption)


# ----------- BLIP-2 captioning -----------
def caption_blip2(img):
    proc = blip2_proc_base
    model = blip2_model_base

    inputs = proc(images=img, return_tensors="pt").to(device)

    out = model.generate(
        **inputs,
        do_sample=True,
        top_p=0.9,
        temperature=0.8,              # slightly lower temp = more consistent
        max_new_tokens=150,
        repetition_penalty=1.6,       # strong repetition control
        no_repeat_ngram_size=6,
        length_penalty=1.1,
        num_beams=5
    )

    caption = proc.decode(out[0], skip_special_tokens=True)
    return clean_caption(caption)


# ----------- InstructBLIP captioning -----------
def caption_instructblip(
    img,
    instruction=(
        "Describe this image in detailed, natural language. "
        "Include people, objects, background, lighting, emotions, and atmosphere. "
        "Avoid repeating phrases or listing the same objects twice."
    )
):
    proc = instruct_proc_base
    model = instruct_model_base

    inputs = proc(images=img, text=instruction, return_tensors="pt").to(device)

    out = model.generate(
        **inputs,
        do_sample=True,
        top_p=0.9,
        temperature=0.85,
        max_new_tokens=180,
        repetition_penalty=1.5,
        no_repeat_ngram_size=6,
        length_penalty=1.2,
        num_beams=6
    )

    caption = proc.decode(out[0], skip_special_tokens=True)
    return clean_caption(caption)

# --- Image Preprocessing -------------
from PIL import ImageOps, ImageEnhance, Image

def preprocess_image(image, max_size=512):
    """
    Preprocess input image before feeding into captioning models.
    - Correct EXIF orientation
    - Enhance contrast and sharpness
    - Resize to a manageable resolution (maintaining aspect ratio)
    - Ensure RGB format
    """
    try:
        # Auto-orient (handles rotated EXIF tags)
        image = ImageOps.exif_transpose(image)

        # Enhance visual quality slightly
        image = ImageEnhance.Contrast(image).enhance(1.1)
        image = ImageEnhance.Sharpness(image).enhance(1.1)

        # Resize to avoid GPU overload, maintaining aspect ratio
        image.thumbnail((max_size, max_size))

        # Convert to RGB mode if not already
        image = image.convert("RGB")

        return image

    except Exception as e:
        print(f"‚ö†Ô∏è Image preprocessing failed: {e}")
        return image.convert("RGB")

# --- Rank captions using CLIP ---
def rank_captions_by_clip(img, captions, return_all=False):
    """
    Rank multiple generated captions for an image using CLIP similarity.
    Returns the most relevant caption (and optionally all scores).
    """

    if not captions or len(captions) == 0:
        raise ValueError("‚ùå No captions provided for CLIP ranking.")

    try:
        # Ensure image is in RGB mode
        if isinstance(img, Image.Image):
            img = img.convert("RGB")

        # Encode image-text pairs
        inputs = clip_proc(
            text=captions,
            images=img,
            return_tensors="pt",
            padding=True
        ).to(device)

        with torch.no_grad():
            outputs = clip_model(**inputs)

        # Get CLIP similarity logits
        logits = outputs.logits_per_image.float()  # use float32 for stability
        probs = logits.softmax(dim=1).cpu().numpy()[0]

        # Determine best caption
        best_idx = int(probs.argmax())
        best_caption = captions[best_idx]

        if return_all:
            # Sort scores in descending order
            scores = {cap: float(score) for cap, score in zip(captions, probs)}
            sorted_scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))
            return best_caption, sorted_scores
        else:
            return best_caption

    except Exception as e:
        print(f"‚ö†Ô∏è CLIP ranking failed: {e}")
        # Return first caption as fallback
        return captions[0] if captions else ""

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from nltk.tokenize import word_tokenize
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.spice.spice import Spice

try:
    nltk.data.find("corpora/wordnet")
except LookupError:
    nltk.download("wordnet")
    nltk.download("omw-1.4")
    nltk.download("punkt")
def save_current_plot(filename):
    """Save the current matplotlib figure and return its path."""
    plt.tight_layout()
    plt.savefig(filename, bbox_inches="tight", dpi=200)
    plt.close()
    return filename

# ---------- Pseudo-reference metrics ----------
def evaluate_self_consistency(captions, best_caption):
    """
    Evaluate BLEU, METEOR, CIDEr, and SPICE using other generated captions
    as pseudo-references for the best caption.
    """
    others = [c for c in captions if c != best_caption]

    smoothie = SmoothingFunction().method4
    bleu = sentence_bleu([o.split() for o in others],
                         best_caption.split(),
                         smoothing_function=smoothie)
    meteor = meteor_score(
        [word_tokenize(o) for o in others],
        word_tokenize(best_caption)
    )

    gts = {0: others}
    res = {0: [best_caption]}

    # CIDEr
    cider_scorer = Cider()
    cider, _ = cider_scorer.compute_score(gts, res)

    # SPICE
    try:
        spice_scorer = Spice()
        spice, _ = spice_scorer.compute_score(gts, res)
    except Exception as e:
        print("‚ö†Ô∏è SPICE computation failed:", e)
        spice = 0.0

    return {
        "BLEU": round(bleu, 4),
        "METEOR": round(meteor, 4),
        "CIDEr": round(cider, 4),
        "SPICE": round(spice, 4)
    }

# ---------- Hybrid caption pipeline ----------
def hybrid_caption(img_path):
    """
    Generate captions using multiple BLIP variants (base only),
    rank them using CLIP, compute pseudo-reference metrics,
    and visualize results.
    """
    img = preprocess_image(img_path)
    print("\nüöÄ Generating captions from base models...")

    captions, models, times = [], [], []

    # ---- BLIP (base)
    start = time.time(); cap = caption_blip(img); t = time.time() - start
    captions.append(cap); models.append("BLIP (base)"); times.append(t)

    # ---- BLIP-large (base)
    start = time.time(); cap = caption_blip_large(img); t = time.time() - start
    captions.append(cap); models.append("BLIP-large (base)"); times.append(t)

    # ---- BLIP-2 (base)
    start = time.time(); cap = caption_blip2(img); t = time.time() - start
    captions.append(cap); models.append("BLIP-2 (base)"); times.append(t)

    # ---- InstructBLIP (base)
    start = time.time(); cap = caption_instructblip(img); t = time.time() - start
    captions.append(cap); models.append("InstructBLIP (base)"); times.append(t)

    # ---- Caption stats
    lengths = [len(c.split()) for c in captions]
    readability = [flesch_reading_ease(c) for c in captions]

    # ---- Rank captions using CLIP
    best_caption, clip_score_dict = rank_captions_by_clip(img, captions, return_all=True)
    clip_scores = [clip_score_dict[c] for c in captions]

    # ---- DataFrame
    df = pd.DataFrame({
        "Model": models,
        "Caption": captions,
        "CLIP Score": clip_scores,
        "Length": lengths,
        "Time (s)": [round(t, 2) for t in times],
        "Readability": [round(r, 2) for r in readability]
    }).sort_values(by=["CLIP Score", "Length", "Time (s)"],ascending=[False, True, True]).reset_index(drop=True)

    # ---- Select best
    best_row = df.iloc[0]
    best_caption, best_model, best_score = best_row["Caption"], best_row["Model"], best_row["CLIP Score"]

    # ---- Compute pseudo-reference metrics
    metrics = evaluate_self_consistency(captions, best_caption)
    reference_models = [m for m in models if m != best_model]

    # ---- Display textual summary
    print(f"\nüèÜ Best Caption ({best_model}) ‚Äî CLIP Score: {best_score:.3f}")
    print(f"üìù {best_caption}\n")
    print("üìå Pseudo-reference metrics (computed against other generated captions):")
    print(f"   - BLEU:   {metrics['BLEU']} ‚Üí Lexical overlap")
    print(f"   - METEOR: {metrics['METEOR']} ‚Üí Semantic similarity")
    print(f"   - CIDEr:  {metrics['CIDEr']} ‚Üí Consensus relevance")
    print(f"   - SPICE:  {metrics['SPICE']} ‚Üí Scene and object-level alignment\n")

    # ---- Plots
    plt.figure()
    plot_clip_scores(df)
    clip_plot_path = "clip_scores.png"
    plt.savefig(clip_plot_path, bbox_inches="tight", dpi=150)
    plt.show()
    print(f"üìä Saved CLIP score plot ‚Üí {clip_plot_path}")

    plt.figure()
    plot_text_metrics(metrics, best_model, best_score, reference_models)
    metrics_plot_path = "text_metrics.png"
    plt.savefig(metrics_plot_path, bbox_inches="tight", dpi=150)
    plt.show()
    print(f"üìà Saved pseudo-reference metrics plot ‚Üí {metrics_plot_path}")

    # ---- Caption table & visualization
    display_caption_table(df)
    show_result(img, best_caption, best_model, best_score)

    # ---- Intelligent conclusion
    conclusion = interpret_metrics(metrics, best_model, best_score, reference_models)
    print(f"\nüîç Interpretation: {conclusion}\n")

    return best_caption, df, metrics, clip_plot_path, metrics_plot_path

# ---------- Plot pseudo-reference metrics ----------
def plot_text_metrics(metrics, best_model, best_score, reference_models):
    plt.figure(figsize=(7, 4))
    plt.bar(metrics.keys(), metrics.values(), color="mediumseagreen")

    refs_str = ", ".join(reference_models)
    plt.title(
        f"Pseudo-Reference Metrics (Self-Consistency)\n"
        f"Best: {best_model} (CLIP={best_score:.3f}) | Refs: {refs_str}",
        fontsize=11
    )

    plt.ylabel("Score")
    plt.ylim(0, 1)
    for i, (k, v) in enumerate(metrics.items()):
        plt.text(i, v + 0.02, f"{v:.3f}", ha="center", fontsize=9)
    plt.tight_layout()

# ---------- Metric interpretation ----------
def interpret_metrics(metrics, best_model, best_score, reference_models):
    """
    Generate a natural-language interpretation of the pseudo-reference metrics.
    """
    bleu, meteor, cider, spice = (
        metrics["BLEU"], metrics["METEOR"], metrics["CIDEr"], metrics["SPICE"]
    )
    refs_str = ", ".join(reference_models)

    if bleu > 0.6 and meteor > 0.5 and cider > 0.7 and spice > 0.4:
        return (f"{best_model} (CLIP={best_score:.3f}) achieved strong agreement with other models "
                f"({refs_str}), showing high lexical, semantic, and scene-level consistency.")
    elif meteor > 0.4 and cider > 0.5 and spice > 0.3:
        return (f"{best_model} (CLIP={best_score:.3f}) aligns semantically and visually "
                f"with {refs_str}, suggesting good scene understanding.")
    else:
        return (f"{best_model} (CLIP={best_score:.3f}) diverges from {refs_str}, "
                f"indicating a possibly unique or creative interpretation of the image.")

# ---------- Existing helpers ----------
def plot_clip_scores(df):
    plt.figure(figsize=(8, 4))
    plt.bar(df["Model"], df["CLIP Score"], color="cornflowerblue")
    plt.title("CLIP Similarity Score by Model")
    plt.ylabel("CLIP Score")
    plt.xlabel("Model")
    plt.ylim(0, 1)
    for i, v in enumerate(df["CLIP Score"]):
        plt.text(i, v + 0.01, f"{v:.3f}", ha="center", fontsize=9)
    plt.tight_layout()

def display_caption_table(df):
    df_show = df.copy()
    df_show["Caption"] = df_show["Caption"].apply(lambda x: f"<b>{x}</b>")
    display(HTML(df_show.to_html(escape=False, index=False)))

def show_result(img, best_caption, best_model, best_score):
    plt.figure(figsize=(5, 5))
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"üèÜ {best_model}\n{best_caption}\n(CLIP: {best_score:.3f})", fontsize=10)
    plt.tight_layout()
    plt.show()

# OCR
def extract_text_from_image(img):
    img = preprocess_image(img).convert("L")
    result = pytesseract.image_to_data(img, output_type=Output.DICT)
    text = " ".join([word for word in result["text"] if word.strip() != ""])
    return text.strip() if text else "‚ö†Ô∏è No text detected."

def visualize_ocr_boxes(img):
    data = pytesseract.image_to_data(img, output_type=Output.DICT)
    draw = ImageDraw.Draw(img)
    for i in range(len(data['text'])):
        if int(data['conf'][i]) > 60:
            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
            draw.rectangle([x, y, x+w, y+h], outline="red", width=2)
    display(img)

def run_object_detection(img):
    """
    Runs YOLOv8 object detection on a PIL image.
    Returns a summary string and the annotated PIL image.
    """
    # Save input temporarily
    img_path = "temp_input.jpg"
    img.save(img_path)

    # Run detection
    results = yolo_model.predict(img_path, conf=0.4, verbose=False)

    # Extract detected objects
    detections = results[0].boxes.data
    names = yolo_model.names
    detected_objects = []

    for box in detections:
        cls = int(box[5].item())
        label = names[cls]
        detected_objects.append(label)

    # Create annotated image
    annotated_img_array = results[0].plot()  # NumPy array
    annotated_img = Image.fromarray(annotated_img_array)

    # Create summary
    if detected_objects:
        detected_summary = "Detected objects: " + ", ".join(sorted(set(detected_objects)))
    else:
        detected_summary = "‚ö†Ô∏è No objects detected."

    return detected_summary, annotated_img

# ======================
# Webcam Capture Function
# ======================
def take_photo(filename='photo.jpg', quality=0.9):
    display(Javascript('''
        async function takePhoto(quality) {
          const div = document.createElement('div');
          const video = document.createElement('video');
          const captureButton = document.createElement('button');
          captureButton.textContent = 'üì∏ Capture Image';
          captureButton.style.fontSize = '16px';
          captureButton.style.marginTop = '10px';
          div.appendChild(video);
          div.appendChild(captureButton);
          document.body.appendChild(div);

          const stream = await navigator.mediaDevices.getUserMedia({video: true});
          video.srcObject = stream;
          await video.play();

          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          await new Promise(resolve => captureButton.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);

          stream.getTracks().forEach(t => t.stop());
          div.remove();

          return canvas.toDataURL('image/jpeg', quality);
        }
    '''))

    try:
        data = eval_js('takePhoto({})'.format(quality))
        binary = b64decode(data.split(',')[1])
        with open(filename, 'wb') as f:
            f.write(binary)
        return filename
    except Exception as e:
        raise RuntimeError(f"Camera error: {e}")

# ======================
# UI Setup
# ======================
input_method = widgets.RadioButtons(
    options=['Upload File', 'Use Camera'],
    description='Choose input:',
    disabled=False
)
confirm_button = widgets.Button(description="Confirm Selection", button_style='success')

out_upload = widgets.Output()

display(input_method, confirm_button, out_upload)


img = None  # placeholder

# ======================
# Main Handler
# ======================
def on_confirm_clicked(b):
    global img
    out_upload.clear_output(wait=True)

    with out_upload:
        method = input_method.value

        if method == 'Upload File':
            print("üìÅ Please upload an image file...")
            uploaded = files.upload()
            if not uploaded:
                print("‚ùå No file uploaded.")
                return
            image_path = list(uploaded.keys())[0]
            img = (Image.open(image_path).convert("RGB")).resize((400, 400))
            img = preprocess_image(img)
            print(f"‚úÖ Uploaded: {image_path}")
            display(img)

        elif method == 'Use Camera':
            print("üé• Starting camera...")
            try:
                filename = take_photo()
                img = Image.open(filename).convert("RGB").resize((400, 400))
                img = preprocess_image(img)
                print(f"‚úÖ Photo captured and saved as {filename}")
                display(IPImage(filename))
            except Exception as e:
                print(str(e))

# ======================
# Bind the button
# ======================
confirm_button.on_click(on_confirm_clicked)

#mounting gdrive for VQA
from google.colab import drive
drive.mount('/content/drive')

# ========================
# VISUAL QUESTION ANSWERING FUNCTION
# ========================
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from transformers import BlipProcessor, BlipForQuestionAnswering
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load BLIP VQA model once
vqa_processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
vqa_model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to(device)

# Core function to run VQA
def run_vqa(image, question):
    inputs = vqa_processor(image, question, return_tensors="pt").to(device)
    out = vqa_model.generate(**inputs, max_new_tokens=30)
    answer = vqa_processor.decode(out[0], skip_special_tokens=True)
    return answer

# ======================
# Service Buttons
# ======================
generate_caption_button = widgets.Button(description="ü™Ñ Generate Caption", button_style='info')
vqa_button = widgets.Button(description="Ask Question (VQA)", button_style='warning')
obj_detect_button = widgets.Button(description="Object Detection / Similarity", button_style='danger')
extract_text_button = widgets.Button(description="Extract Text", button_style='success')

out_service_buttons = widgets.Output()
out_ocr = widgets.Output()

question_box = widgets.Text(
    value='',
    placeholder='Ask a question about the image...',
    description='Question:',
    disabled=False
)
question_box.on_submit(lambda change: vqa(None))

display(generate_caption_button, vqa_button, obj_detect_button, out_service_buttons, extract_text_button, out_ocr)

from gtts import gTTS
from IPython.display import Audio, Image as IPyImage, display, HTML

# Global variables
last_caption = None
df_metrics = None
metric_values = None
clip_plot_path = None
metrics_plot_path = None

metrics_box = widgets.Output()



# ---------- Audio feedback ----------
def speak_caption(caption):
    try:
        tts = gTTS(caption)
        tts.save("caption.mp3")
        display(Audio("caption.mp3", autoplay=True))
    except Exception as e:
        print(f"(Audio unavailable: {e})")


# ---------- Caption generation ----------
def generate_caption(b):
    global img, last_caption, df_metrics, metric_values, clip_plot_path, metrics_plot_path

    with out_service_buttons:
        clear_output(wait=True)
        if img is None:
            print("‚ö†Ô∏è Please upload or capture an image first.")
            return

        print("üß† Generating caption, please wait...\n")

        # The updated hybrid_caption returns 5 values
        last_caption, df_metrics, metric_values, clip_plot_path, metrics_plot_path = hybrid_caption(img)

        # Speak caption
        speak_caption(last_caption)

        # Display final caption summary
        display(widgets.HTML(f"<b>‚ú® Final Caption:</b> {last_caption}"))
        print("\nüèÜ Best Caption Visualization:")
        plt.figure(figsize=(5,5))
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"{last_caption}\n(CLIP Best)", fontsize=10)
        plt.tight_layout()
        plt.show()


# ---------- Visual Question Answering ----------
def vqa(b):
    with out_service_buttons:
        clear_output(wait=True)
        display(question_box)

        if img is None:
            print("‚ö†Ô∏è Please upload or capture an image first!")
            return

        question = question_box.value.strip()
        if not question:
            print("‚ö†Ô∏è Please type a question above!")
            return

        print(f"üß† Processing question: {question}")
        answer = run_vqa(img, question)
        display(img)
        print(f"üí¨ Answer: {answer}")


# ---------- Object Detection ----------
def run_obj_detect(b):
    with out_service_buttons:
        clear_output(wait=True)
        if img is None:
            print("‚ö†Ô∏è Please upload or capture an image first.")
            return
        print("üîé Running object detection...\n")
        summary, annotated_img = run_object_detection(img)
        print(summary)
        display(annotated_img)


# ---------- OCR ----------
def run_ocr(b):
    global img
    with out_service_buttons:
        clear_output(wait=True)
        if img is None:
            print("‚ö†Ô∏è Upload or capture an image first.")
            return
        print("üîç Extracting text via OCR...")
        text = extract_text_from_image(img)
        print("\nüìù Extracted Text:\n")
        print(text)
        visualize_ocr_boxes(img)


# ---------- Connect buttons ----------
generate_caption_button.on_click(generate_caption)
vqa_button.on_click(vqa)
obj_detect_button.on_click(run_obj_detect)
extract_text_button.on_click(run_ocr)

"""# Gradio Based UI"""

#Phase3-Final UI adaptation
import gradio as gr
from gtts import gTTS
import tempfile
import pandas as pd
from PIL import Image
import IPython
import webbrowser

# -----------------------------
# Helper: Placeholder audio
# -----------------------------
def placeholder_audio(text="Please upload an image first"):
    tts = gTTS(text)
    tmp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(tmp_audio.name)
    return tmp_audio.name

# =======================================================
# ü™Ñ GRADIO FUNCTION WRAPPERS
# =======================================================

def gr_generate_caption(img):
    """Generate caption, metrics, and visuals."""
    if img is None:
        audio = placeholder_audio()
        return (
            "‚ö†Ô∏è Please upload an image.",
            None,               # Image placeholder
            audio,
            None,               # CLIP plot placeholder
            None,               # Metric plot placeholder
            "<i>Metrics will appear here after uploading an image.</i>"
        )

    # Run your existing caption pipeline
    best_caption, df_metrics, metric_values, clip_plot_path, metrics_plot_path = hybrid_caption(img)

    # Create audio using gTTS
    tts = gTTS(best_caption)
    tmp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(tmp_audio.name)

    # Convert metrics DataFrame to HTML
    html_table = df_metrics.to_html(escape=False, index=False)

    return (
        best_caption,
        img,                # Original uploaded image
        tmp_audio.name,
        clip_plot_path,     # Actual CLIP ranking plot
        metrics_plot_path,  # Actual metric scores plot
        html_table
    )

def gr_vqa(img, question):
    if img is None:
        return "‚ö†Ô∏è Please upload an image first."
    if not question.strip():
        return "‚ö†Ô∏è Please enter a question."
    return run_vqa(img, question)

def gr_obj_detect(img):
    if img is None:
        return "‚ö†Ô∏è Please upload an image first.", None
    return run_object_detection(img)

def gr_ocr(img):
    if img is None:
        return "‚ö†Ô∏è Please upload an image first.", None
    text = extract_text_from_image(img)
    vis_img = visualize_ocr_boxes(img)
    return text, vis_img

# =======================================================
# üñ•Ô∏è GRADIO INTERFACE
# =======================================================
with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue")) as demo:
    gr.Markdown("# üß† Vision Intelligence Suite")
    gr.Markdown("Generate captions, answer questions, detect objects, and extract text.")

    with gr.Tabs():
        # Caption Generation
        with gr.TabItem("ü™Ñ Caption Generation"):
            img_input = gr.Image(label="Upload Image", type="pil")
            cap_btn = gr.Button("Generate Caption")

            with gr.Row():
                cap_text = gr.Textbox(label="Generated Caption", lines=2, placeholder="‚ö†Ô∏è Please upload an image")
                cap_audio = gr.Audio(label="Spoken Caption", autoplay=True, value=placeholder_audio())

            cap_img = gr.Image(label="Image Preview", interactive=False)
            clip_plot = gr.Image(label="CLIP Ranking Plot", interactive=False)
            metric_plot = gr.Image(label="Metric Scores Plot", interactive=False)
            table_html = gr.HTML(label="Detailed Metrics Table", value="<i>Metrics will appear here after uploading an image.</i>")

            cap_btn.click(
                gr_generate_caption,
                inputs=img_input,
                outputs=[cap_text, cap_img, cap_audio, clip_plot, metric_plot, table_html]
            )

        # VQA Tab
        with gr.TabItem("‚ùì Visual Question Answering"):
            img_vqa = gr.Image(label="Upload Image", type="pil")
            question_box = gr.Textbox(label="Question", placeholder="What is happening in this image?")
            vqa_btn = gr.Button("Ask")
            vqa_out = gr.Textbox(label="Answer", placeholder="‚ö†Ô∏è Please upload an image first")
            vqa_btn.click(gr_vqa, inputs=[img_vqa, question_box], outputs=vqa_out)

        # Object Detection
        with gr.TabItem("üîé Object Detection"):
            img_det = gr.Image(label="Upload Image", type="pil")
            det_btn = gr.Button("Run Object Detection")
            det_out = gr.Textbox(label="Detected Objects", placeholder="‚ö†Ô∏è Please upload an image first")
            det_img = gr.Image(label="Annotated Image", interactive=False)
            det_btn.click(gr_obj_detect, inputs=img_det, outputs=[det_out, det_img])

        # OCR
        with gr.TabItem("üìù OCR Text Extraction"):
            img_ocr = gr.Image(label="Upload Image", type="pil")
            ocr_btn = gr.Button("Extract Text")
            ocr_text = gr.Textbox(label="Extracted Text", lines=5, placeholder="‚ö†Ô∏è Please upload an image first")
            ocr_btn.click(gr_ocr, inputs=img_ocr, outputs=[ocr_text])

# Launch app
demo.launch(share=True, inbrowser=True,inline=False)