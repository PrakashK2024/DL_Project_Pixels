{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "# Check if the directory exists before cloning\n",
        "import os\n",
        "if not os.path.exists('pycocoevalcap'):\n",
        "    !git clone https://github.com/salaniz/pycocoevalcap.git\n",
        "else:\n",
        "    print(\"Directory 'pycocoevalcap' already exists. Skipping clone.\")\n",
        "\n",
        "\n",
        "# Navigate into the directory and install\n",
        "# Add a check to ensure the directory exists before changing into it\n",
        "if os.path.exists('pycocoevalcap'):\n",
        "    !cd pycocoevalcap && python setup.py install\n",
        "else:\n",
        "    print(\"Directory 'pycocoevalcap' not found. Skipping installation.\")\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "# Download NLTK data if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "# Install openjdk\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-11-jre-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIv5nwvimTv8",
        "outputId": "2f37bd40-c2ca-4b4c-adfe-5d063d6c949c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'pycocoevalcap' already exists. Skipping clone.\n",
            "running install\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing pycocoevalcap.egg-info/PKG-INFO\n",
            "writing dependency_links to pycocoevalcap.egg-info/dependency_links.txt\n",
            "writing requirements to pycocoevalcap.egg-info/requires.txt\n",
            "writing top-level names to pycocoevalcap.egg-info/top_level.txt\n",
            "reading manifest file 'pycocoevalcap.egg-info/SOURCES.txt'\n",
            "writing manifest file 'pycocoevalcap.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying ./eval.py -> build/lib/pycocoevalcap\n",
            "copying ./tokenizer/ptbtokenizer.py -> build/lib/pycocoevalcap/tokenizer\n",
            "copying ./spice/spice.py -> build/lib/pycocoevalcap/spice\n",
            "copying ./spice/__init__.py -> build/lib/pycocoevalcap/spice\n",
            "copying ./spice/get_stanford_models.py -> build/lib/pycocoevalcap/spice\n",
            "copying ./example/coco_eval_example.py -> build/lib/pycocoevalcap/example\n",
            "copying ./meteor/meteor.py -> build/lib/pycocoevalcap/meteor\n",
            "copying ./bleu/bleu_scorer.py -> build/lib/pycocoevalcap/bleu\n",
            "copying ./bleu/bleu.py -> build/lib/pycocoevalcap/bleu\n",
            "copying ./rouge/rouge.py -> build/lib/pycocoevalcap/rouge\n",
            "copying ./cider/cider.py -> build/lib/pycocoevalcap/cider\n",
            "copying ./cider/cider_scorer.py -> build/lib/pycocoevalcap/cider\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap\n",
            "copying ./build/lib/pycocoevalcap/eval.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/tokenizer\n",
            "copying ./build/lib/pycocoevalcap/tokenizer/ptbtokenizer.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/tokenizer\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying ./build/lib/pycocoevalcap/spice/spice.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying ./build/lib/pycocoevalcap/spice/__init__.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying ./build/lib/pycocoevalcap/spice/get_stanford_models.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/example\n",
            "copying ./build/lib/pycocoevalcap/example/coco_eval_example.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/example\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor\n",
            "copying ./build/lib/pycocoevalcap/meteor/meteor.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/bleu\n",
            "copying ./build/lib/pycocoevalcap/bleu/bleu_scorer.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/bleu\n",
            "copying ./build/lib/pycocoevalcap/bleu/bleu.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/bleu\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/rouge\n",
            "copying ./build/lib/pycocoevalcap/rouge/rouge.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/rouge\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/cider\n",
            "copying ./build/lib/pycocoevalcap/cider/cider.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/cider\n",
            "copying ./build/lib/pycocoevalcap/cider/cider_scorer.py -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/cider\n",
            "copying ./tokenizer/stanford-corenlp-3.4.1.jar -> build/lib/pycocoevalcap/tokenizer\n",
            "copying ./spice/spice-1.0.jar -> build/lib/pycocoevalcap/spice\n",
            "copying ./meteor/meteor-1.5.jar -> build/lib/pycocoevalcap/meteor\n",
            "copying ./spice/lib/json-simple-1.1.1.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/junit-4.12.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/Meteor-1.5.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/javassist-3.19.0-GA.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/fst-2.47.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/stanford-corenlp-3.6.0-models.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/guava-19.0.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/ejml-0.23.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/stanford-corenlp-3.6.0.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/lmdbjni-0.4.6.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/lmdbjni-linux64-0.4.6.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/objenesis-2.4.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/lmdbjni-win64-0.4.6.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/slf4j-api-1.7.12.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/slf4j-simple-1.7.21.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/lmdbjni-osx64-0.4.6.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/SceneGraphParser-1.0.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/hamcrest-core-1.3.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./spice/lib/jackson-core-2.5.3.jar -> build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/tokenizer/stanford-corenlp-3.4.1.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/tokenizer\n",
            "copying ./build/lib/pycocoevalcap/spice/spice-1.0.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying ./build/lib/pycocoevalcap/meteor/meteor-1.5.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/json-simple-1.1.1.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/junit-4.12.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/Meteor-1.5.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/javassist-3.19.0-GA.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/fst-2.47.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/guava-19.0.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/ejml-0.23.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/lmdbjni-0.4.6.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/lmdbjni-linux64-0.4.6.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/objenesis-2.4.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/lmdbjni-win64-0.4.6.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/slf4j-api-1.7.12.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/slf4j-simple-1.7.21.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/lmdbjni-osx64-0.4.6.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/SceneGraphParser-1.0.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/hamcrest-core-1.3.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying ./build/lib/pycocoevalcap/spice/lib/jackson-core-2.5.3.jar -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "creating build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor/data\n",
            "copying ./build/lib/pycocoevalcap/meteor/data/paraphrase-en.gz -> build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor/data\n",
            "copying ./meteor/data/paraphrase-en.gz -> build/lib/pycocoevalcap/meteor/data\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/tokenizer\n",
            "copying build/lib/pycocoevalcap/tokenizer/stanford-corenlp-3.4.1.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/tokenizer\n",
            "copying build/lib/pycocoevalcap/tokenizer/ptbtokenizer.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/tokenizer\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/spice/spice.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/spice/spice-1.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/spice/__init__.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/spice/get_stanford_models.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/json-simple-1.1.1.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/junit-4.12.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/Meteor-1.5.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/javassist-3.19.0-GA.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/fst-2.47.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/stanford-corenlp-3.6.0-models.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/guava-19.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/ejml-0.23.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/stanford-corenlp-3.6.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/lmdbjni-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/lmdbjni-linux64-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/objenesis-2.4.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/lmdbjni-win64-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/slf4j-api-1.7.12.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/slf4j-simple-1.7.21.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/lmdbjni-osx64-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/SceneGraphParser-1.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/hamcrest-core-1.3.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/spice/lib/jackson-core-2.5.3.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/spice/lib\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/example\n",
            "copying build/lib/pycocoevalcap/example/coco_eval_example.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/example\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/tokenizer\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/tokenizer/stanford-corenlp-3.4.1.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/tokenizer\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/tokenizer/ptbtokenizer.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/tokenizer\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/spice.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/spice-1.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/__init__.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/get_stanford_models.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/json-simple-1.1.1.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/junit-4.12.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/Meteor-1.5.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/javassist-3.19.0-GA.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/fst-2.47.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/guava-19.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/ejml-0.23.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/lmdbjni-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/lmdbjni-linux64-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/objenesis-2.4.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/lmdbjni-win64-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/slf4j-api-1.7.12.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/slf4j-simple-1.7.21.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/lmdbjni-osx64-0.4.6.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/SceneGraphParser-1.0.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/hamcrest-core-1.3.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/spice/lib/jackson-core-2.5.3.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/lib\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/example\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/example/coco_eval_example.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/example\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/meteor\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/meteor/data\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor/data/paraphrase-en.gz -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/meteor/data\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor/meteor.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/meteor\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/meteor/meteor-1.5.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/meteor\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/bleu\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/bleu/bleu_scorer.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/bleu\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/bleu/bleu.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/bleu\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/rouge\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/rouge/rouge.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/rouge\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/eval.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/cider\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/cider/cider.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/cider\n",
            "copying build/lib/pycocoevalcap/build/lib/pycocoevalcap/cider/cider_scorer.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/cider\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/meteor\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/meteor/data\n",
            "copying build/lib/pycocoevalcap/meteor/data/paraphrase-en.gz -> build/bdist.linux-x86_64/egg/pycocoevalcap/meteor/data\n",
            "copying build/lib/pycocoevalcap/meteor/meteor.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/meteor\n",
            "copying build/lib/pycocoevalcap/meteor/meteor-1.5.jar -> build/bdist.linux-x86_64/egg/pycocoevalcap/meteor\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/bleu\n",
            "copying build/lib/pycocoevalcap/bleu/bleu_scorer.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/bleu\n",
            "copying build/lib/pycocoevalcap/bleu/bleu.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/bleu\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/rouge\n",
            "copying build/lib/pycocoevalcap/rouge/rouge.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/rouge\n",
            "copying build/lib/pycocoevalcap/eval.py -> build/bdist.linux-x86_64/egg/pycocoevalcap\n",
            "creating build/bdist.linux-x86_64/egg/pycocoevalcap/cider\n",
            "copying build/lib/pycocoevalcap/cider/cider.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/cider\n",
            "copying build/lib/pycocoevalcap/cider/cider_scorer.py -> build/bdist.linux-x86_64/egg/pycocoevalcap/cider\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/tokenizer/ptbtokenizer.py to ptbtokenizer.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/spice/spice.py to spice.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/spice/__init__.py to __init__.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/spice/get_stanford_models.py to get_stanford_models.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/example/coco_eval_example.py to coco_eval_example.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/tokenizer/ptbtokenizer.py to ptbtokenizer.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/spice.py to spice.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/__init__.py to __init__.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/spice/get_stanford_models.py to get_stanford_models.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/example/coco_eval_example.py to coco_eval_example.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/meteor/meteor.py to meteor.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/bleu/bleu_scorer.py to bleu_scorer.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/bleu/bleu.py to bleu.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/rouge/rouge.py to rouge.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/eval.py to eval.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/cider/cider.py to cider.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/build/lib/pycocoevalcap/cider/cider_scorer.py to cider_scorer.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/meteor/meteor.py to meteor.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/bleu/bleu_scorer.py to bleu_scorer.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/bleu/bleu.py to bleu.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/rouge/rouge.py to rouge.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/eval.py to eval.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/cider/cider.py to cider.cpython-312.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocoevalcap/cider/cider_scorer.py to cider_scorer.cpython-312.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocoevalcap.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocoevalcap.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocoevalcap.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocoevalcap.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocoevalcap.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "pycocoevalcap.build.lib.pycocoevalcap.meteor.__pycache__.meteor.cpython-312: module references __file__\n",
            "pycocoevalcap.build.lib.pycocoevalcap.spice.__pycache__.get_stanford_models.cpython-312: module references __file__\n",
            "pycocoevalcap.build.lib.pycocoevalcap.spice.__pycache__.spice.cpython-312: module references __file__\n",
            "pycocoevalcap.build.lib.pycocoevalcap.tokenizer.__pycache__.ptbtokenizer.cpython-312: module references __file__\n",
            "pycocoevalcap.meteor.__pycache__.meteor.cpython-312: module references __file__\n",
            "pycocoevalcap.spice.__pycache__.get_stanford_models.cpython-312: module references __file__\n",
            "pycocoevalcap.spice.__pycache__.spice.cpython-312: module references __file__\n",
            "pycocoevalcap.tokenizer.__pycache__.ptbtokenizer.cpython-312: module references __file__\n",
            "creating 'dist/pycocoevalcap-1.2-py3.12.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pycocoevalcap-1.2-py3.12.egg\n",
            "removing '/usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg\n",
            "Extracting pycocoevalcap-1.2-py3.12.egg to /usr/local/lib/python3.12/dist-packages\n",
            "Adding pycocoevalcap 1.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg\n",
            "Processing dependencies for pycocoevalcap==1.2\n",
            "Searching for pycocotools==2.0.10\n",
            "Best match: pycocotools 2.0.10\n",
            "Adding pycocotools 2.0.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for numpy==2.0.2\n",
            "Best match: numpy 2.0.2\n",
            "Adding numpy 2.0.2 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing numpy-config script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Finished processing dependencies for pycocoevalcap==1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ub\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to r2u.stat.illinois\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to r2u.stat.illinois\r                                                                               \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jre-headless is already the newest version (11.0.29+7-1ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive to access Kaggle API key\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory for Kaggle credentials\n",
        "# Check if the directory exists before creating\n",
        "if not os.path.exists('~/.kaggle'):\n",
        "    !mkdir ~/.kaggle'\n",
        "else:\n",
        "    print(\"Directory '~/.kaggle' already exists. Skipping mkdir.\")\n",
        "\n",
        "\n",
        "# Copy the Kaggle API key from Google Drive\n",
        "# Check if the source file exists before copying\n",
        "kaggle_json_path = '/content/drive/MyDrive/kaggle.json'\n",
        "if os.path.exists(kaggle_json_path):\n",
        "    !cp {kaggle_json_path} ~/.kaggle/\n",
        "    # Set permissions for the Kaggle API key\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(\"Kaggle API key copied and permissions set.\")\n",
        "else:\n",
        "    print(f\"Kaggle API key not found at {kaggle_json_path}. Skipping copy and chmod.\")\n",
        "    print(\"Please ensure your 'kaggle.json' file is in your Google Drive's root directory.\")\n",
        "\n",
        "\n",
        "# Download the dataset\n",
        "# Check if the zip file already exists before downloading\n",
        "dataset_zip_path = 'flickr8k.zip'\n",
        "if not os.path.exists(dataset_zip_path):\n",
        "    # Add a check for the Kaggle API key file before attempting to download\n",
        "    if os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):\n",
        "        !kaggle datasets download -d adityarajput1999/flickr8k\n",
        "    else:\n",
        "        print(\"Kaggle API key not found. Skipping dataset download.\")\n",
        "else:\n",
        "    print(f\"Dataset zip file '{dataset_zip_path}' already exists. Skipping download.\")\n",
        "\n",
        "\n",
        "# Unzip the dataset\n",
        "# Check if the zip file exists before unzipping\n",
        "dataset_extract_dir = '/content/archive'\n",
        "if os.path.exists(dataset_zip_path):\n",
        "    # Check if the destination directory already contains extracted files\n",
        "    # This is a simple check; a more robust check might look for specific files/folders\n",
        "    if not os.path.exists(dataset_extract_dir) or not os.listdir(dataset_extract_dir):\n",
        "        !unzip {dataset_zip_path} -d {dataset_extract_dir}\n",
        "        print(f\"Dataset unzipped to {dataset_extract_dir}.\")\n",
        "    else:\n",
        "        print(f\"Dataset already seems to be extracted in {dataset_extract_dir}. Skipping unzip.\")\n",
        "else:\n",
        "    print(f\"Dataset zip file '{dataset_zip_path}' not found. Skipping unzip.\")\n",
        "\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "# Check if the directory exists before listing contents\n",
        "if os.path.exists(dataset_extract_dir):\n",
        "    print(f\"\\nContents of {dataset_extract_dir}:\")\n",
        "    !ls {dataset_extract_dir}\n",
        "else:\n",
        "    print(f\"Extracted dataset directory '{dataset_extract_dir}' not found.\")"
      ],
      "metadata": {
        "id": "ccTuhmucGax6",
        "outputId": "c65b2713-838d-43c3-d41b-ada8670c1488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
            "Kaggle API key not found at /content/drive/MyDrive/kaggle.json. Skipping copy and chmod.\n",
            "Please ensure your 'kaggle.json' file is in your Google Drive's root directory.\n",
            "Kaggle API key not found. Skipping dataset download.\n",
            "Dataset zip file 'flickr8k.zip' not found. Skipping unzip.\n",
            "Extracted dataset directory '/content/archive' not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhfiw3_xTJiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348238ba-a5fe-470d-fbf4-a1cac8005a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pycocoevalcap-1.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q gTTS\n",
        "!apt install -q tesseract-ocr\n",
        "!pip install -q pytesseract\n",
        "!pip install -q ultralytics\n",
        "!pip install -q textstat\n",
        "!pip install -q transformers datasets accelerate torch torchvision\n",
        "!pip install -q peft\n",
        "!pip install -q numpy scipy nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S8E9z1xLcvp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dbe8f1-af10-4915-8800-6d721388dcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJcqCDeZDiJf"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# ðŸ“¦ Imports & Setup\n",
        "# ==========================\n",
        "from nltk.tokenize import word_tokenize\n",
        "from PIL import Image\n",
        "import torch\n",
        "from google.colab import files\n",
        "from huggingface_hub import logging as hf_logging\n",
        "hf_logging.set_verbosity_error()\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import files\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import display, Javascript, Image as IPImage, clear_output\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "from PIL import ImageDraw\n",
        "import torch\n",
        "import nltk\n",
        "from IPython.display import display, HTML\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "from transformers import (\n",
        "    BlipProcessor, BlipForConditionalGeneration,\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    CLIPProcessor, CLIPModel\n",
        ")\n",
        "from PIL import Image, ImageOps, ImageEnhance,ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "from textstat import flesch_reading_ease\n",
        "from IPython.display import display, HTML\n",
        "from transformers import (\n",
        "    BlipProcessor, BlipForConditionalGeneration,\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from PIL import Image\n",
        "import torch, random, os\n",
        "os.environ[\"HF_SAFE_TENSORS_MODE\"] = \"FALSE\"\n",
        "import os\n",
        "os.environ[\"SPICE_JAR\"] = \"/usr/local/lib/python3.10/dist-packages/pycocoevalcap/spice/Spice.jar\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBVrMMT2flE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d95662-b351-4f30-da1d-bf8508d7d46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Paths ---\n",
        "folder_path = \"/content/drive/MyDrive/archive/Images\"\n",
        "captions_path = \"/content/drive/MyDrive/archive/captions.txt\"\n",
        "\n",
        "# --- Read captions.txt properly ---\n",
        "data = []\n",
        "with open(captions_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            parts = line.split(',', 1)  # comma-separated\n",
        "            if len(parts) == 2:\n",
        "                image_id, caption = parts\n",
        "                image_id = image_id.split('#')[0].strip()  # remove #0, #1, etc.\n",
        "                caption = caption.strip()\n",
        "                data.append([image_id, caption])\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"image_name\", \"caption_text\"])\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "df = df.groupby(\"image_name\")[\"caption_text\"].apply(list).reset_index()\n",
        "df[\"image_path\"] = df[\"image_name\"].apply(lambda x: os.path.join(folder_path, x))\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "X7n183zLC251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "7398980d-f82b-4a93-f108-ffa045f1bcd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  image_name  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1001773457_577c3a7d70.jpg   \n",
              "\n",
              "                                        caption_text  \\\n",
              "0  [A child in a pink dress is climbing up a set ...   \n",
              "1  [A black dog and a spotted dog are fighting, A...   \n",
              "\n",
              "                                          image_path  \n",
              "0  /content/drive/MyDrive/archive/Images/10002682...  \n",
              "1  /content/drive/MyDrive/archive/Images/10017734...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0063a03c-cedc-42cb-967e-9f5bcaaef72f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>caption_text</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>[A child in a pink dress is climbing up a set ...</td>\n",
              "      <td>/content/drive/MyDrive/archive/Images/10002682...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1001773457_577c3a7d70.jpg</td>\n",
              "      <td>[A black dog and a spotted dog are fighting, A...</td>\n",
              "      <td>/content/drive/MyDrive/archive/Images/10017734...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0063a03c-cedc-42cb-967e-9f5bcaaef72f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0063a03c-cedc-42cb-967e-9f5bcaaef72f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0063a03c-cedc-42cb-967e-9f5bcaaef72f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5aebf76c-4ddd-4e71-9240-c52686e8b259\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5aebf76c-4ddd-4e71-9240-c52686e8b259')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5aebf76c-4ddd-4e71-9240-c52686e8b259 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8092,\n  \"fields\": [\n    {\n      \"column\": \"image_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8092,\n        \"samples\": [\n          \"3139895886_5a6d495b13.jpg\",\n          \"3133825703_359a0c414d.jpg\",\n          \"244910177_7c4ec3f65b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8092,\n        \"samples\": [\n          \"/content/drive/MyDrive/archive/Images/3139895886_5a6d495b13.jpg\",\n          \"/content/drive/MyDrive/archive/Images/3133825703_359a0c414d.jpg\",\n          \"/content/drive/MyDrive/archive/Images/244910177_7c4ec3f65b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on device: {device}\")\n",
        "\n",
        "# ===============================\n",
        "# --- Dataset class ---\n",
        "# ===============================\n",
        "class FlickrDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, processor):\n",
        "        self.df = dataframe\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row[\"image_path\"]\n",
        "        caption = random.choice(row[\"caption_text\"])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=caption,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
        "        return {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "# ===============================\n",
        "# --- Mixed precision fine-tuning function ---\n",
        "# ===============================\n",
        "def fine_tune_blip_amp(df, model_name, processor_class, model_class, output_dir,\n",
        "                       epochs=1, batch_size=1, lr=5e-5, max_train_samples=200,\n",
        "                       grad_accum_steps=8):\n",
        "\n",
        "    print(f\"\\nðŸš€ Fine-tuning {model_name} with mixed precision ...\")\n",
        "\n",
        "    # Processor & model\n",
        "    processor = processor_class.from_pretrained(model_name)\n",
        "    model = model_class.from_pretrained(model_name).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    train_df = df.sample(max_train_samples, random_state=42).reset_index(drop=True)\n",
        "    dataset = FlickrDataset(train_df, processor)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scaler = torch.cuda.amp.GradScaler()  # for mixed precision\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        epoch_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, batch in enumerate(tqdm(loader)):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            with torch.cuda.amp.autocast():  # mixed precision\n",
        "                outputs = model(\n",
        "                    input_ids=inputs[\"input_ids\"],\n",
        "                    attention_mask=inputs[\"attention_mask\"],\n",
        "                    pixel_values=inputs[\"pixel_values\"],\n",
        "                    labels=inputs[\"labels\"]\n",
        "                )\n",
        "                loss = outputs.loss / grad_accum_steps  # normalize loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (step + 1) % grad_accum_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item() * grad_accum_steps\n",
        "\n",
        "        print(f\"Epoch {epoch+1} loss: {epoch_loss/len(loader):.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    model.save_pretrained(output_dir, safe_serialization=False)\n",
        "    processor.save_pretrained(output_dir)\n",
        "    print(f\"ðŸ’¾ Saved fine-tuned model: {output_dir}\")\n",
        "\n",
        "    # Clean up GPU memory\n",
        "    del model, processor, dataset, loader, optimizer, scaler\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "8crTnqeienQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5a8658-36e3-4d04-d309-3d5292438b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Load Pretrained Models (Base Only) -----------\n",
        "\n",
        "print(\"ðŸ”„ Loading pretrained models ...\")\n",
        "\n",
        "from transformers import (\n",
        "    BlipProcessor, BlipForConditionalGeneration,\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    CLIPProcessor, CLIPModel\n",
        ")\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ==================== BLIP Models ====================\n",
        "\n",
        "blip_proc_base = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model_base = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        ").to(device)\n",
        "\n",
        "blip_proc_large = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model_large = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-large\"\n",
        ").to(device)\n",
        "\n",
        "# ==================== BLIP-2 Models ====================\n",
        "\n",
        "blip2_proc_base = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "blip2_model_base = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-flan-t5-xl\"\n",
        ").to(device)\n",
        "\n",
        "# ==================== InstructBLIP Models ====================\n",
        "\n",
        "instruct_proc_base = Blip2Processor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
        "instruct_model_base = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/instructblip-flan-t5-xl\"\n",
        ").to(device)\n",
        "\n",
        "# ==================== CLIP + YOLO ====================\n",
        "\n",
        "clip_proc = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "yolo_model = YOLO(\"yolov8s.pt\")\n",
        "\n",
        "print(\"âœ… All base models loaded successfully!\")"
      ],
      "metadata": {
        "id": "ZTRrLL--IKZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "91114718ed234fdcb81cf519e931b5ff",
            "9b7908c3bf5e4c58a8fd7fc63dcd6ddc",
            "d0056c1f2e2542038003cdd18d3414bf",
            "d747180c818b4143a50808765d44a21f",
            "aaebe5c3c07d45739650793ebb476534",
            "384d96e5630449cc86c470e1c2495a5b",
            "95657d28fc5f4ba1967ce667cf3bbd14",
            "6fede1747edf43cc931321a58ee4bce6",
            "349d9c79a672408fb10298967db2a547",
            "f5eaf4b44e5f4812955c5549dca876f6",
            "eb7281a018804d8ba0b5d8feba2de367",
            "48da80c1dfca4f1391cfaa8d5015ed77",
            "2ab8d1debdae4f4ab0972f7634dc70fc",
            "91c227ba3c6049ab9a8374d1006bca2b",
            "c4cbd99125a14b5990478b395b63187e",
            "2186f13afdc647e894eef64d884b4d01",
            "1c66c78101384049b54b53b885615684",
            "d016b26d6e874c769f8fe56cb9132144",
            "0dd6424fe3a24034abc5aed16782aee7",
            "5967192fc08447f5bd491754e0d2acf0",
            "696d4c56289a4b4ea06d44b30e4c06d6",
            "a1eb1fa4d82d413a91b93f9e2fdf5f4a"
          ]
        },
        "outputId": "be9d76d4-4cd1-4926-ca94-34786e85e937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Loading pretrained models ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91114718ed234fdcb81cf519e931b5ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48da80c1dfca4f1391cfaa8d5015ed77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All base models loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# ----------- Utility: Clean up repetitive captions -----------\n",
        "def clean_caption(text):\n",
        "    \"\"\"Removes repeated words, duplicate phrases, and extra spaces.\"\"\"\n",
        "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # remove consecutive duplicate words\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)             # collapse multiple spaces\n",
        "    text = text.replace(\" : \", \": \")                # fix spacing after colons\n",
        "    return text.strip().capitalize()\n",
        "\n",
        "\n",
        "# ----------- BLIP captioning -----------\n",
        "def caption_blip(img):\n",
        "    proc = blip_proc_base\n",
        "    model = blip_model_base\n",
        "\n",
        "\n",
        "    inputs = proc(images=img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.85,\n",
        "        max_new_tokens=120,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=6,\n",
        "        length_penalty=1.1,\n",
        "        num_beams=5\n",
        "    )\n",
        "\n",
        "    caption = proc.decode(out[0], skip_special_tokens=True)\n",
        "    return clean_caption(caption)\n",
        "\n",
        "\n",
        "# ----------- BLIP-large captioning -----------\n",
        "def caption_blip_large(img):\n",
        "    proc = blip_proc_large\n",
        "    model = blip_model_large\n",
        "\n",
        "    inputs = proc(images=img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.85,\n",
        "        max_new_tokens=150,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=6,\n",
        "        length_penalty=1.2,\n",
        "        num_beams=6\n",
        "    )\n",
        "\n",
        "    caption = proc.decode(out[0], skip_special_tokens=True)\n",
        "    return clean_caption(caption)\n",
        "\n",
        "\n",
        "# ----------- BLIP-2 captioning -----------\n",
        "def caption_blip2(img):\n",
        "    proc = blip2_proc_base\n",
        "    model = blip2_model_base\n",
        "\n",
        "    inputs = proc(images=img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,              # slightly lower temp = more consistent\n",
        "        max_new_tokens=150,\n",
        "        repetition_penalty=1.6,       # strong repetition control\n",
        "        no_repeat_ngram_size=6,\n",
        "        length_penalty=1.1,\n",
        "        num_beams=5\n",
        "    )\n",
        "\n",
        "    caption = proc.decode(out[0], skip_special_tokens=True)\n",
        "    return clean_caption(caption)\n",
        "\n",
        "\n",
        "# ----------- InstructBLIP captioning -----------\n",
        "def caption_instructblip(\n",
        "    img,\n",
        "    instruction=(\n",
        "        \"Describe this image in detailed, natural language. \"\n",
        "        \"Include people, objects, background, lighting, emotions, and atmosphere. \"\n",
        "        \"Avoid repeating phrases or listing the same objects twice.\"\n",
        "    )\n",
        "):\n",
        "    proc = instruct_proc_base\n",
        "    model = instruct_model_base\n",
        "\n",
        "    inputs = proc(images=img, text=instruction, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.85,\n",
        "        max_new_tokens=180,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=6,\n",
        "        length_penalty=1.2,\n",
        "        num_beams=6\n",
        "    )\n",
        "\n",
        "    caption = proc.decode(out[0], skip_special_tokens=True)\n",
        "    return clean_caption(caption)\n"
      ],
      "metadata": {
        "id": "__NexYbYMZ1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnlVXaRlkmMV"
      },
      "outputs": [],
      "source": [
        "# --- Image Preprocessing -------------\n",
        "from PIL import ImageOps, ImageEnhance, Image\n",
        "\n",
        "def preprocess_image(image, max_size=512):\n",
        "    \"\"\"\n",
        "    Preprocess input image before feeding into captioning models.\n",
        "    - Correct EXIF orientation\n",
        "    - Enhance contrast and sharpness\n",
        "    - Resize to a manageable resolution (maintaining aspect ratio)\n",
        "    - Ensure RGB format\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Auto-orient (handles rotated EXIF tags)\n",
        "        image = ImageOps.exif_transpose(image)\n",
        "\n",
        "        # Enhance visual quality slightly\n",
        "        image = ImageEnhance.Contrast(image).enhance(1.1)\n",
        "        image = ImageEnhance.Sharpness(image).enhance(1.1)\n",
        "\n",
        "        # Resize to avoid GPU overload, maintaining aspect ratio\n",
        "        image.thumbnail((max_size, max_size))\n",
        "\n",
        "        # Convert to RGB mode if not already\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Image preprocessing failed: {e}\")\n",
        "        return image.convert(\"RGB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_XM4UQSfh1W"
      },
      "outputs": [],
      "source": [
        "# --- Rank captions using CLIP ---\n",
        "def rank_captions_by_clip(img, captions, return_all=False):\n",
        "    \"\"\"\n",
        "    Rank multiple generated captions for an image using CLIP similarity.\n",
        "    Returns the most relevant caption (and optionally all scores).\n",
        "    \"\"\"\n",
        "\n",
        "    if not captions or len(captions) == 0:\n",
        "        raise ValueError(\"âŒ No captions provided for CLIP ranking.\")\n",
        "\n",
        "    try:\n",
        "        # Ensure image is in RGB mode\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = img.convert(\"RGB\")\n",
        "\n",
        "        # Encode image-text pairs\n",
        "        inputs = clip_proc(\n",
        "            text=captions,\n",
        "            images=img,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = clip_model(**inputs)\n",
        "\n",
        "        # Get CLIP similarity logits\n",
        "        logits = outputs.logits_per_image.float()  # use float32 for stability\n",
        "        probs = logits.softmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "        # Determine best caption\n",
        "        best_idx = int(probs.argmax())\n",
        "        best_caption = captions[best_idx]\n",
        "\n",
        "        if return_all:\n",
        "            # Sort scores in descending order\n",
        "            scores = {cap: float(score) for cap, score in zip(captions, probs)}\n",
        "            sorted_scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
        "            return best_caption, sorted_scores\n",
        "        else:\n",
        "            return best_caption\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ CLIP ranking failed: {e}\")\n",
        "        # Return first caption as fallback\n",
        "        return captions[0] if captions else \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "\n",
        "try:\n",
        "    nltk.data.find(\"corpora/wordnet\")\n",
        "except LookupError:\n",
        "    nltk.download(\"wordnet\")\n",
        "    nltk.download(\"omw-1.4\")\n",
        "    nltk.download(\"punkt\")\n",
        "def save_current_plot(filename):\n",
        "    \"\"\"Save the current matplotlib figure and return its path.\"\"\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, bbox_inches=\"tight\", dpi=200)\n",
        "    plt.close()\n",
        "    return filename\n",
        "\n",
        "# ---------- Pseudo-reference metrics ----------\n",
        "def evaluate_self_consistency(captions, best_caption):\n",
        "    \"\"\"\n",
        "    Evaluate BLEU, METEOR, CIDEr, and SPICE using other generated captions\n",
        "    as pseudo-references for the best caption.\n",
        "    \"\"\"\n",
        "    others = [c for c in captions if c != best_caption]\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu = sentence_bleu([o.split() for o in others],\n",
        "                         best_caption.split(),\n",
        "                         smoothing_function=smoothie)\n",
        "    meteor = meteor_score(\n",
        "        [word_tokenize(o) for o in others],\n",
        "        word_tokenize(best_caption)\n",
        "    )\n",
        "\n",
        "    gts = {0: others}\n",
        "    res = {0: [best_caption]}\n",
        "\n",
        "    # CIDEr\n",
        "    cider_scorer = Cider()\n",
        "    cider, _ = cider_scorer.compute_score(gts, res)\n",
        "\n",
        "    # SPICE\n",
        "    try:\n",
        "        spice_scorer = Spice()\n",
        "        spice, _ = spice_scorer.compute_score(gts, res)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ SPICE computation failed:\", e)\n",
        "        spice = 0.0\n",
        "\n",
        "    return {\n",
        "        \"BLEU\": round(bleu, 4),\n",
        "        \"METEOR\": round(meteor, 4),\n",
        "        \"CIDEr\": round(cider, 4),\n",
        "        \"SPICE\": round(spice, 4)\n",
        "    }\n",
        "\n",
        "# ---------- Hybrid caption pipeline ----------\n",
        "def hybrid_caption(img_path):\n",
        "    \"\"\"\n",
        "    Generate captions using multiple BLIP variants (base only),\n",
        "    rank them using CLIP, compute pseudo-reference metrics,\n",
        "    and visualize results.\n",
        "    \"\"\"\n",
        "    img = preprocess_image(img_path)\n",
        "    print(\"\\nðŸš€ Generating captions from base models...\")\n",
        "\n",
        "    captions, models, times = [], [], []\n",
        "\n",
        "    # ---- BLIP (base)\n",
        "    start = time.time(); cap = caption_blip(img); t = time.time() - start\n",
        "    captions.append(cap); models.append(\"BLIP (base)\"); times.append(t)\n",
        "\n",
        "    # ---- BLIP-large (base)\n",
        "    start = time.time(); cap = caption_blip_large(img); t = time.time() - start\n",
        "    captions.append(cap); models.append(\"BLIP-large (base)\"); times.append(t)\n",
        "\n",
        "    # ---- BLIP-2 (base)\n",
        "    start = time.time(); cap = caption_blip2(img); t = time.time() - start\n",
        "    captions.append(cap); models.append(\"BLIP-2 (base)\"); times.append(t)\n",
        "\n",
        "    # ---- InstructBLIP (base)\n",
        "    start = time.time(); cap = caption_instructblip(img); t = time.time() - start\n",
        "    captions.append(cap); models.append(\"InstructBLIP (base)\"); times.append(t)\n",
        "\n",
        "    # ---- Caption stats\n",
        "    lengths = [len(c.split()) for c in captions]\n",
        "    readability = [flesch_reading_ease(c) for c in captions]\n",
        "\n",
        "    # ---- Rank captions using CLIP\n",
        "    best_caption, clip_score_dict = rank_captions_by_clip(img, captions, return_all=True)\n",
        "    clip_scores = [clip_score_dict[c] for c in captions]\n",
        "\n",
        "    # ---- DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        \"Model\": models,\n",
        "        \"Caption\": captions,\n",
        "        \"CLIP Score\": clip_scores,\n",
        "        \"Length\": lengths,\n",
        "        \"Time (s)\": [round(t, 2) for t in times],\n",
        "        \"Readability\": [round(r, 2) for r in readability]\n",
        "    }).sort_values(by=[\"CLIP Score\", \"Length\", \"Time (s)\"],ascending=[False, True, True]).reset_index(drop=True)\n",
        "\n",
        "    # ---- Select best\n",
        "    best_row = df.iloc[0]\n",
        "    best_caption, best_model, best_score = best_row[\"Caption\"], best_row[\"Model\"], best_row[\"CLIP Score\"]\n",
        "\n",
        "    # ---- Compute pseudo-reference metrics\n",
        "    metrics = evaluate_self_consistency(captions, best_caption)\n",
        "    reference_models = [m for m in models if m != best_model]\n",
        "\n",
        "    # ---- Display textual summary\n",
        "    print(f\"\\nðŸ† Best Caption ({best_model}) â€” CLIP Score: {best_score:.3f}\")\n",
        "    print(f\"ðŸ“ {best_caption}\\n\")\n",
        "    print(\"ðŸ“Œ Pseudo-reference metrics (computed against other generated captions):\")\n",
        "    print(f\"   - BLEU:   {metrics['BLEU']} â†’ Lexical overlap\")\n",
        "    print(f\"   - METEOR: {metrics['METEOR']} â†’ Semantic similarity\")\n",
        "    print(f\"   - CIDEr:  {metrics['CIDEr']} â†’ Consensus relevance\")\n",
        "    print(f\"   - SPICE:  {metrics['SPICE']} â†’ Scene and object-level alignment\\n\")\n",
        "\n",
        "    # ---- Plots\n",
        "    plt.figure()\n",
        "    plot_clip_scores(df)\n",
        "    clip_plot_path = \"clip_scores.png\"\n",
        "    plt.savefig(clip_plot_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"ðŸ“Š Saved CLIP score plot â†’ {clip_plot_path}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plot_text_metrics(metrics, best_model, best_score, reference_models)\n",
        "    metrics_plot_path = \"text_metrics.png\"\n",
        "    plt.savefig(metrics_plot_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"ðŸ“ˆ Saved pseudo-reference metrics plot â†’ {metrics_plot_path}\")\n",
        "\n",
        "    # ---- Caption table & visualization\n",
        "    display_caption_table(df)\n",
        "    show_result(img, best_caption, best_model, best_score)\n",
        "\n",
        "    # ---- Intelligent conclusion\n",
        "    conclusion = interpret_metrics(metrics, best_model, best_score, reference_models)\n",
        "    print(f\"\\nðŸ” Interpretation: {conclusion}\\n\")\n",
        "\n",
        "    return best_caption, df, metrics, clip_plot_path, metrics_plot_path\n",
        "\n",
        "# ---------- Plot pseudo-reference metrics ----------\n",
        "def plot_text_metrics(metrics, best_model, best_score, reference_models):\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.bar(metrics.keys(), metrics.values(), color=\"mediumseagreen\")\n",
        "\n",
        "    refs_str = \", \".join(reference_models)\n",
        "    plt.title(\n",
        "        f\"Pseudo-Reference Metrics (Self-Consistency)\\n\"\n",
        "        f\"Best: {best_model} (CLIP={best_score:.3f}) | Refs: {refs_str}\",\n",
        "        fontsize=11\n",
        "    )\n",
        "\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    for i, (k, v) in enumerate(metrics.items()):\n",
        "        plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontsize=9)\n",
        "    plt.tight_layout()\n",
        "\n",
        "# ---------- Metric interpretation ----------\n",
        "def interpret_metrics(metrics, best_model, best_score, reference_models):\n",
        "    \"\"\"\n",
        "    Generate a natural-language interpretation of the pseudo-reference metrics.\n",
        "    \"\"\"\n",
        "    bleu, meteor, cider, spice = (\n",
        "        metrics[\"BLEU\"], metrics[\"METEOR\"], metrics[\"CIDEr\"], metrics[\"SPICE\"]\n",
        "    )\n",
        "    refs_str = \", \".join(reference_models)\n",
        "\n",
        "    if bleu > 0.6 and meteor > 0.5 and cider > 0.7 and spice > 0.4:\n",
        "        return (f\"{best_model} (CLIP={best_score:.3f}) achieved strong agreement with other models \"\n",
        "                f\"({refs_str}), showing high lexical, semantic, and scene-level consistency.\")\n",
        "    elif meteor > 0.4 and cider > 0.5 and spice > 0.3:\n",
        "        return (f\"{best_model} (CLIP={best_score:.3f}) aligns semantically and visually \"\n",
        "                f\"with {refs_str}, suggesting good scene understanding.\")\n",
        "    else:\n",
        "        return (f\"{best_model} (CLIP={best_score:.3f}) diverges from {refs_str}, \"\n",
        "                f\"indicating a possibly unique or creative interpretation of the image.\")\n",
        "\n",
        "# ---------- Existing helpers ----------\n",
        "def plot_clip_scores(df):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(df[\"Model\"], df[\"CLIP Score\"], color=\"cornflowerblue\")\n",
        "    plt.title(\"CLIP Similarity Score by Model\")\n",
        "    plt.ylabel(\"CLIP Score\")\n",
        "    plt.xlabel(\"Model\")\n",
        "    plt.ylim(0, 1)\n",
        "    for i, v in enumerate(df[\"CLIP Score\"]):\n",
        "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=9)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def display_caption_table(df):\n",
        "    df_show = df.copy()\n",
        "    df_show[\"Caption\"] = df_show[\"Caption\"].apply(lambda x: f\"<b>{x}</b>\")\n",
        "    display(HTML(df_show.to_html(escape=False, index=False)))\n",
        "\n",
        "def show_result(img, best_caption, best_model, best_score):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"ðŸ† {best_model}\\n{best_caption}\\n(CLIP: {best_score:.3f})\", fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DSris8vrSY1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cafad72-e174-40e4-cc52-6166b2d182fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_-ScqdiZLel"
      },
      "outputs": [],
      "source": [
        "# OCR\n",
        "def extract_text_from_image(img):\n",
        "    img = preprocess_image(img).convert(\"L\")\n",
        "    result = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
        "    text = \" \".join([word for word in result[\"text\"] if word.strip() != \"\"])\n",
        "    return text.strip() if text else \"âš ï¸ No text detected.\"\n",
        "\n",
        "def visualize_ocr_boxes(img):\n",
        "    data = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for i in range(len(data['text'])):\n",
        "        if int(data['conf'][i]) > 60:\n",
        "            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "            draw.rectangle([x, y, x+w, y+h], outline=\"red\", width=2)\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HBE-raKaUuO"
      },
      "outputs": [],
      "source": [
        "def run_object_detection(img):\n",
        "    \"\"\"\n",
        "    Runs YOLOv8 object detection on a PIL image.\n",
        "    Returns a summary string and the annotated PIL image.\n",
        "    \"\"\"\n",
        "    # Save input temporarily\n",
        "    img_path = \"temp_input.jpg\"\n",
        "    img.save(img_path)\n",
        "\n",
        "    # Run detection\n",
        "    results = yolo_model.predict(img_path, conf=0.4, verbose=False)\n",
        "\n",
        "    # Extract detected objects\n",
        "    detections = results[0].boxes.data\n",
        "    names = yolo_model.names\n",
        "    detected_objects = []\n",
        "\n",
        "    for box in detections:\n",
        "        cls = int(box[5].item())\n",
        "        label = names[cls]\n",
        "        detected_objects.append(label)\n",
        "\n",
        "    # Create annotated image\n",
        "    annotated_img_array = results[0].plot()  # NumPy array\n",
        "    annotated_img = Image.fromarray(annotated_img_array)\n",
        "\n",
        "    # Create summary\n",
        "    if detected_objects:\n",
        "        detected_summary = \"Detected objects: \" + \", \".join(sorted(set(detected_objects)))\n",
        "    else:\n",
        "        detected_summary = \"âš ï¸ No objects detected.\"\n",
        "\n",
        "    return detected_summary, annotated_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iNFwTPnD2k6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "361d6a6695d148ce8d7aa0b35af1b9e3",
            "5f8f675891cc4554834adafcd831a7cb",
            "6be87a109dfb4192958bee3310548ded",
            "99a77a2106ad489daf87d7400336cfe9",
            "5d3d5dffced54b6a8cfbf18850130ecd",
            "c1942741869e46579aadc129d93588cb",
            "903ff1e55c6541b1964ce5d77f038981",
            "3a4c74dcdce54140bb76f4b3c7577c6e"
          ]
        },
        "outputId": "a19af355-0883-4f63-f281-e52e5ad3f323"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RadioButtons(description='Choose input:', options=('Upload File', 'Use Camera'), value='Upload File')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "361d6a6695d148ce8d7aa0b35af1b9e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Confirm Selection', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99a77a2106ad489daf87d7400336cfe9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "903ff1e55c6541b1964ce5d77f038981"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ======================\n",
        "# Webcam Capture Function\n",
        "# ======================\n",
        "def take_photo(filename='photo.jpg', quality=0.9):\n",
        "    display(Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "          const div = document.createElement('div');\n",
        "          const video = document.createElement('video');\n",
        "          const captureButton = document.createElement('button');\n",
        "          captureButton.textContent = 'ðŸ“¸ Capture Image';\n",
        "          captureButton.style.fontSize = '16px';\n",
        "          captureButton.style.marginTop = '10px';\n",
        "          div.appendChild(video);\n",
        "          div.appendChild(captureButton);\n",
        "          document.body.appendChild(div);\n",
        "\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "          await new Promise(resolve => captureButton.onclick = resolve);\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "          stream.getTracks().forEach(t => t.stop());\n",
        "          div.remove();\n",
        "\n",
        "          return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    '''))\n",
        "\n",
        "    try:\n",
        "        data = eval_js('takePhoto({})'.format(quality))\n",
        "        binary = b64decode(data.split(',')[1])\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(binary)\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Camera error: {e}\")\n",
        "\n",
        "# ======================\n",
        "# UI Setup\n",
        "# ======================\n",
        "input_method = widgets.RadioButtons(\n",
        "    options=['Upload File', 'Use Camera'],\n",
        "    description='Choose input:',\n",
        "    disabled=False\n",
        ")\n",
        "confirm_button = widgets.Button(description=\"Confirm Selection\", button_style='success')\n",
        "\n",
        "out_upload = widgets.Output()\n",
        "\n",
        "display(input_method, confirm_button, out_upload)\n",
        "\n",
        "\n",
        "img = None  # placeholder\n",
        "\n",
        "# ======================\n",
        "# Main Handler\n",
        "# ======================\n",
        "def on_confirm_clicked(b):\n",
        "    global img\n",
        "    out_upload.clear_output(wait=True)\n",
        "\n",
        "    with out_upload:\n",
        "        method = input_method.value\n",
        "\n",
        "        if method == 'Upload File':\n",
        "            print(\"ðŸ“ Please upload an image file...\")\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                print(\"âŒ No file uploaded.\")\n",
        "                return\n",
        "            image_path = list(uploaded.keys())[0]\n",
        "            img = (Image.open(image_path).convert(\"RGB\")).resize((400, 400))\n",
        "            img = preprocess_image(img)\n",
        "            print(f\"âœ… Uploaded: {image_path}\")\n",
        "            display(img)\n",
        "\n",
        "        elif method == 'Use Camera':\n",
        "            print(\"ðŸŽ¥ Starting camera...\")\n",
        "            try:\n",
        "                filename = take_photo()\n",
        "                img = Image.open(filename).convert(\"RGB\").resize((400, 400))\n",
        "                img = preprocess_image(img)\n",
        "                print(f\"âœ… Photo captured and saved as {filename}\")\n",
        "                display(IPImage(filename))\n",
        "            except Exception as e:\n",
        "                print(str(e))\n",
        "\n",
        "# ======================\n",
        "# Bind the button\n",
        "# ======================\n",
        "confirm_button.on_click(on_confirm_clicked)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting gdrive for VQA\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "omPTXjkphYGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31456df0-fe3a-47b0-f855-f42ea756ad9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYem71lb2xzv"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# VISUAL QUESTION ANSWERING FUNCTION\n",
        "# ========================\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load BLIP VQA model once\n",
        "vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
        "\n",
        "# Core function to run VQA\n",
        "def run_vqa(image, question):\n",
        "    inputs = vqa_processor(image, question, return_tensors=\"pt\").to(device)\n",
        "    out = vqa_model.generate(**inputs, max_new_tokens=30)\n",
        "    answer = vqa_processor.decode(out[0], skip_special_tokens=True)\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Service Buttons\n",
        "# ======================\n",
        "generate_caption_button = widgets.Button(description=\"ðŸª„ Generate Caption\", button_style='info')\n",
        "vqa_button = widgets.Button(description=\"Ask Question (VQA)\", button_style='warning')\n",
        "obj_detect_button = widgets.Button(description=\"Object Detection / Similarity\", button_style='danger')\n",
        "extract_text_button = widgets.Button(description=\"Extract Text\", button_style='success')\n",
        "\n",
        "out_service_buttons = widgets.Output()\n",
        "out_ocr = widgets.Output()\n",
        "\n",
        "question_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ask a question about the image...',\n",
        "    description='Question:',\n",
        "    disabled=False\n",
        ")\n",
        "question_box.on_submit(lambda change: vqa(None))\n",
        "\n",
        "display(generate_caption_button, vqa_button, obj_detect_button, out_service_buttons, extract_text_button, out_ocr)\n",
        "\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, Image as IPyImage, display, HTML\n",
        "\n",
        "# Global variables\n",
        "last_caption = None\n",
        "df_metrics = None\n",
        "metric_values = None\n",
        "clip_plot_path = None\n",
        "metrics_plot_path = None\n",
        "\n",
        "metrics_box = widgets.Output()\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Audio feedback ----------\n",
        "def speak_caption(caption):\n",
        "    try:\n",
        "        tts = gTTS(caption)\n",
        "        tts.save(\"caption.mp3\")\n",
        "        display(Audio(\"caption.mp3\", autoplay=True))\n",
        "    except Exception as e:\n",
        "        print(f\"(Audio unavailable: {e})\")\n",
        "\n",
        "\n",
        "# ---------- Caption generation ----------\n",
        "def generate_caption(b):\n",
        "    global img, last_caption, df_metrics, metric_values, clip_plot_path, metrics_plot_path\n",
        "\n",
        "    with out_service_buttons:\n",
        "        clear_output(wait=True)\n",
        "        if img is None:\n",
        "            print(\"âš ï¸ Please upload or capture an image first.\")\n",
        "            return\n",
        "\n",
        "        print(\"ðŸ§  Generating caption, please wait...\\n\")\n",
        "\n",
        "        # The updated hybrid_caption returns 5 values\n",
        "        last_caption, df_metrics, metric_values, clip_plot_path, metrics_plot_path = hybrid_caption(img)\n",
        "\n",
        "        # Speak caption\n",
        "        speak_caption(last_caption)\n",
        "\n",
        "        # Display final caption summary\n",
        "        display(widgets.HTML(f\"<b>âœ¨ Final Caption:</b> {last_caption}\"))\n",
        "        print(\"\\nðŸ† Best Caption Visualization:\")\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"{last_caption}\\n(CLIP Best)\", fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ---------- Visual Question Answering ----------\n",
        "def vqa(b):\n",
        "    with out_service_buttons:\n",
        "        clear_output(wait=True)\n",
        "        display(question_box)\n",
        "\n",
        "        if img is None:\n",
        "            print(\"âš ï¸ Please upload or capture an image first!\")\n",
        "            return\n",
        "\n",
        "        question = question_box.value.strip()\n",
        "        if not question:\n",
        "            print(\"âš ï¸ Please type a question above!\")\n",
        "            return\n",
        "\n",
        "        print(f\"ðŸ§  Processing question: {question}\")\n",
        "        answer = run_vqa(img, question)\n",
        "        display(img)\n",
        "        print(f\"ðŸ’¬ Answer: {answer}\")\n",
        "\n",
        "\n",
        "# ---------- Object Detection ----------\n",
        "def run_obj_detect(b):\n",
        "    with out_service_buttons:\n",
        "        clear_output(wait=True)\n",
        "        if img is None:\n",
        "            print(\"âš ï¸ Please upload or capture an image first.\")\n",
        "            return\n",
        "        print(\"ðŸ”Ž Running object detection...\\n\")\n",
        "        summary, annotated_img = run_object_detection(img)\n",
        "        print(summary)\n",
        "        display(annotated_img)\n",
        "\n",
        "\n",
        "# ---------- OCR ----------\n",
        "def run_ocr(b):\n",
        "    global img\n",
        "    with out_service_buttons:\n",
        "        clear_output(wait=True)\n",
        "        if img is None:\n",
        "            print(\"âš ï¸ Upload or capture an image first.\")\n",
        "            return\n",
        "        print(\"ðŸ” Extracting text via OCR...\")\n",
        "        text = extract_text_from_image(img)\n",
        "        print(\"\\nðŸ“ Extracted Text:\\n\")\n",
        "        print(text)\n",
        "        visualize_ocr_boxes(img)\n",
        "\n",
        "\n",
        "# ---------- Connect buttons ----------\n",
        "generate_caption_button.on_click(generate_caption)\n",
        "vqa_button.on_click(vqa)\n",
        "obj_detect_button.on_click(run_obj_detect)\n",
        "extract_text_button.on_click(run_ocr)\n"
      ],
      "metadata": {
        "id": "fu7Kh2w8UwkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e79f14da67dc49399d320ac3f55ee301",
            "21227c2a4e32481a95ca988d7104024d",
            "d5e98e33237745e3a65f215500770c03",
            "2b747e77db27407b95b79361102c08a5",
            "f4b68151f4c645c883c0eee22f79b918",
            "ed9c9615b93c45698f52e93ea3f71d5a",
            "4ef107aa59b540deab1e672e7bc18a77",
            "71f680a51b4746358f1147cf2d7c6cab",
            "244a14868f2d4b639ca365a91303d2ae",
            "738af5ed11cf4638a3784db7b1b6a922",
            "9df57b4a3cb0411b95c627e14df33a04",
            "55b4d6d45117494ab58de0465e78a45f",
            "08de785cb6764f6c83c80bbad9ac6c43",
            "f8589615d1a3448e88d22cc9b33efa0b",
            "febe5cefd6644576a1d56cd9f609116b",
            "fb1c180b360e4d0bb2eb17464646462c"
          ],
          "height": 145
        },
        "outputId": "e4b2014f-7f71-487e-c81b-dea131ae3a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='info', description='ðŸª„ Generate Caption', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e79f14da67dc49399d320ac3f55ee301"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Ask Question (VQA)', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b747e77db27407b95b79361102c08a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='danger', description='Object Detection / Similarity', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ef107aa59b540deab1e672e7bc18a77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "738af5ed11cf4638a3784db7b1b6a922"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Extract Text', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55b4d6d45117494ab58de0465e78a45f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "febe5cefd6644576a1d56cd9f609116b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Based UI"
      ],
      "metadata": {
        "id": "Y-nlBcnfgXt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Phase3-Final UI adaptation\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import IPython\n",
        "import webbrowser\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: Placeholder audio\n",
        "# -----------------------------\n",
        "def placeholder_audio(text=\"Please upload an image first\"):\n",
        "    tts = gTTS(text)\n",
        "    tmp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
        "    tts.save(tmp_audio.name)\n",
        "    return tmp_audio.name\n",
        "\n",
        "# =======================================================\n",
        "# ðŸª„ GRADIO FUNCTION WRAPPERS\n",
        "# =======================================================\n",
        "\n",
        "def gr_generate_caption(img):\n",
        "    \"\"\"Generate caption, metrics, and visuals.\"\"\"\n",
        "    if img is None:\n",
        "        audio = placeholder_audio()\n",
        "        return (\n",
        "            \"âš ï¸ Please upload an image.\",\n",
        "            None,               # Image placeholder\n",
        "            audio,\n",
        "            None,               # CLIP plot placeholder\n",
        "            None,               # Metric plot placeholder\n",
        "            \"<i>Metrics will appear here after uploading an image.</i>\"\n",
        "        )\n",
        "\n",
        "    # Run your existing caption pipeline\n",
        "    best_caption, df_metrics, metric_values, clip_plot_path, metrics_plot_path = hybrid_caption(img)\n",
        "\n",
        "    # Create audio using gTTS\n",
        "    tts = gTTS(best_caption)\n",
        "    tmp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
        "    tts.save(tmp_audio.name)\n",
        "\n",
        "    # Convert metrics DataFrame to HTML\n",
        "    html_table = df_metrics.to_html(escape=False, index=False)\n",
        "\n",
        "    return (\n",
        "        best_caption,\n",
        "        img,                # Original uploaded image\n",
        "        tmp_audio.name,\n",
        "        clip_plot_path,     # Actual CLIP ranking plot\n",
        "        metrics_plot_path,  # Actual metric scores plot\n",
        "        html_table\n",
        "    )\n",
        "\n",
        "def gr_vqa(img, question):\n",
        "    if img is None:\n",
        "        return \"âš ï¸ Please upload an image first.\"\n",
        "    if not question.strip():\n",
        "        return \"âš ï¸ Please enter a question.\"\n",
        "    return run_vqa(img, question)\n",
        "\n",
        "def gr_obj_detect(img):\n",
        "    if img is None:\n",
        "        return \"âš ï¸ Please upload an image first.\", None\n",
        "    return run_object_detection(img)\n",
        "\n",
        "def gr_ocr(img):\n",
        "    if img is None:\n",
        "        return \"âš ï¸ Please upload an image first.\", None\n",
        "    text = extract_text_from_image(img)\n",
        "    vis_img = visualize_ocr_boxes(img)\n",
        "    return text, vis_img\n",
        "\n",
        "# =======================================================\n",
        "# ðŸ–¥ï¸ GRADIO INTERFACE\n",
        "# =======================================================\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
        "    gr.Markdown(\"# ðŸ§  Vision Intelligence Suite\")\n",
        "    gr.Markdown(\"Generate captions, answer questions, detect objects, and extract text.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Caption Generation\n",
        "        with gr.TabItem(\"ðŸª„ Caption Generation\"):\n",
        "            img_input = gr.Image(label=\"Upload Image\", type=\"pil\")\n",
        "            cap_btn = gr.Button(\"Generate Caption\")\n",
        "\n",
        "            with gr.Row():\n",
        "                cap_text = gr.Textbox(label=\"Generated Caption\", lines=2, placeholder=\"âš ï¸ Please upload an image\")\n",
        "                cap_audio = gr.Audio(label=\"Spoken Caption\", autoplay=True, value=placeholder_audio())\n",
        "\n",
        "            cap_img = gr.Image(label=\"Image Preview\", interactive=False)\n",
        "            clip_plot = gr.Image(label=\"CLIP Ranking Plot\", interactive=False)\n",
        "            metric_plot = gr.Image(label=\"Metric Scores Plot\", interactive=False)\n",
        "            table_html = gr.HTML(label=\"Detailed Metrics Table\", value=\"<i>Metrics will appear here after uploading an image.</i>\")\n",
        "\n",
        "            cap_btn.click(\n",
        "                gr_generate_caption,\n",
        "                inputs=img_input,\n",
        "                outputs=[cap_text, cap_img, cap_audio, clip_plot, metric_plot, table_html]\n",
        "            )\n",
        "\n",
        "        # VQA Tab\n",
        "        with gr.TabItem(\"â“ Visual Question Answering\"):\n",
        "            img_vqa = gr.Image(label=\"Upload Image\", type=\"pil\")\n",
        "            question_box = gr.Textbox(label=\"Question\", placeholder=\"What is happening in this image?\")\n",
        "            vqa_btn = gr.Button(\"Ask\")\n",
        "            vqa_out = gr.Textbox(label=\"Answer\", placeholder=\"âš ï¸ Please upload an image first\")\n",
        "            vqa_btn.click(gr_vqa, inputs=[img_vqa, question_box], outputs=vqa_out)\n",
        "\n",
        "        # Object Detection\n",
        "        with gr.TabItem(\"ðŸ”Ž Object Detection\"):\n",
        "            img_det = gr.Image(label=\"Upload Image\", type=\"pil\")\n",
        "            det_btn = gr.Button(\"Run Object Detection\")\n",
        "            det_out = gr.Textbox(label=\"Detected Objects\", placeholder=\"âš ï¸ Please upload an image first\")\n",
        "            det_img = gr.Image(label=\"Annotated Image\", interactive=False)\n",
        "            det_btn.click(gr_obj_detect, inputs=img_det, outputs=[det_out, det_img])\n",
        "\n",
        "        # OCR\n",
        "        with gr.TabItem(\"ðŸ“ OCR Text Extraction\"):\n",
        "            img_ocr = gr.Image(label=\"Upload Image\", type=\"pil\")\n",
        "            ocr_btn = gr.Button(\"Extract Text\")\n",
        "            ocr_text = gr.Textbox(label=\"Extracted Text\", lines=5, placeholder=\"âš ï¸ Please upload an image first\")\n",
        "            ocr_btn.click(gr_ocr, inputs=img_ocr, outputs=[ocr_text])\n",
        "\n",
        "# Launch app\n",
        "demo.launch(share=True, inbrowser=True,inline=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK4MGtxjNylt",
        "outputId": "d52d6e00-b6df-4007-f8f9-393a2e102d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bc8e8cd53e7ab9bd80.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91114718ed234fdcb81cf519e931b5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b7908c3bf5e4c58a8fd7fc63dcd6ddc",
              "IPY_MODEL_d0056c1f2e2542038003cdd18d3414bf",
              "IPY_MODEL_d747180c818b4143a50808765d44a21f"
            ],
            "layout": "IPY_MODEL_aaebe5c3c07d45739650793ebb476534"
          }
        },
        "9b7908c3bf5e4c58a8fd7fc63dcd6ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384d96e5630449cc86c470e1c2495a5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_95657d28fc5f4ba1967ce667cf3bbd14",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "d0056c1f2e2542038003cdd18d3414bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fede1747edf43cc931321a58ee4bce6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_349d9c79a672408fb10298967db2a547",
            "value": 2
          }
        },
        "d747180c818b4143a50808765d44a21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5eaf4b44e5f4812955c5549dca876f6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eb7281a018804d8ba0b5d8feba2de367",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡â€‡1.38it/s]"
          }
        },
        "aaebe5c3c07d45739650793ebb476534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384d96e5630449cc86c470e1c2495a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95657d28fc5f4ba1967ce667cf3bbd14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fede1747edf43cc931321a58ee4bce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349d9c79a672408fb10298967db2a547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5eaf4b44e5f4812955c5549dca876f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7281a018804d8ba0b5d8feba2de367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48da80c1dfca4f1391cfaa8d5015ed77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ab8d1debdae4f4ab0972f7634dc70fc",
              "IPY_MODEL_91c227ba3c6049ab9a8374d1006bca2b",
              "IPY_MODEL_c4cbd99125a14b5990478b395b63187e"
            ],
            "layout": "IPY_MODEL_2186f13afdc647e894eef64d884b4d01"
          }
        },
        "2ab8d1debdae4f4ab0972f7634dc70fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c66c78101384049b54b53b885615684",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d016b26d6e874c769f8fe56cb9132144",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "91c227ba3c6049ab9a8374d1006bca2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dd6424fe3a24034abc5aed16782aee7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5967192fc08447f5bd491754e0d2acf0",
            "value": 2
          }
        },
        "c4cbd99125a14b5990478b395b63187e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696d4c56289a4b4ea06d44b30e4c06d6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1eb1fa4d82d413a91b93f9e2fdf5f4a",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡â€‡4.61it/s]"
          }
        },
        "2186f13afdc647e894eef64d884b4d01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c66c78101384049b54b53b885615684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d016b26d6e874c769f8fe56cb9132144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dd6424fe3a24034abc5aed16782aee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5967192fc08447f5bd491754e0d2acf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "696d4c56289a4b4ea06d44b30e4c06d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1eb1fa4d82d413a91b93f9e2fdf5f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "361d6a6695d148ce8d7aa0b35af1b9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "RadioButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "RadioButtonsModel",
            "_options_labels": [
              "Upload File",
              "Use Camera"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "RadioButtonsView",
            "description": "Choose input:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_5f8f675891cc4554834adafcd831a7cb",
            "style": "IPY_MODEL_6be87a109dfb4192958bee3310548ded"
          }
        },
        "5f8f675891cc4554834adafcd831a7cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6be87a109dfb4192958bee3310548ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99a77a2106ad489daf87d7400336cfe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Confirm Selection",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5d3d5dffced54b6a8cfbf18850130ecd",
            "style": "IPY_MODEL_c1942741869e46579aadc129d93588cb",
            "tooltip": ""
          }
        },
        "5d3d5dffced54b6a8cfbf18850130ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1942741869e46579aadc129d93588cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "903ff1e55c6541b1964ce5d77f038981": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3a4c74dcdce54140bb76f4b3c7577c6e",
            "msg_id": "",
            "outputs": []
          }
        },
        "3a4c74dcdce54140bb76f4b3c7577c6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e79f14da67dc49399d320ac3f55ee301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "ðŸª„ Generate Caption",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_21227c2a4e32481a95ca988d7104024d",
            "style": "IPY_MODEL_d5e98e33237745e3a65f215500770c03",
            "tooltip": ""
          }
        },
        "21227c2a4e32481a95ca988d7104024d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e98e33237745e3a65f215500770c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2b747e77db27407b95b79361102c08a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Ask Question (VQA)",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f4b68151f4c645c883c0eee22f79b918",
            "style": "IPY_MODEL_ed9c9615b93c45698f52e93ea3f71d5a",
            "tooltip": ""
          }
        },
        "f4b68151f4c645c883c0eee22f79b918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9c9615b93c45698f52e93ea3f71d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4ef107aa59b540deab1e672e7bc18a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "danger",
            "description": "Object Detection / Similarity",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_71f680a51b4746358f1147cf2d7c6cab",
            "style": "IPY_MODEL_244a14868f2d4b639ca365a91303d2ae",
            "tooltip": ""
          }
        },
        "71f680a51b4746358f1147cf2d7c6cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244a14868f2d4b639ca365a91303d2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "738af5ed11cf4638a3784db7b1b6a922": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9df57b4a3cb0411b95c627e14df33a04",
            "msg_id": "",
            "outputs": []
          }
        },
        "9df57b4a3cb0411b95c627e14df33a04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55b4d6d45117494ab58de0465e78a45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Extract Text",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_08de785cb6764f6c83c80bbad9ac6c43",
            "style": "IPY_MODEL_f8589615d1a3448e88d22cc9b33efa0b",
            "tooltip": ""
          }
        },
        "08de785cb6764f6c83c80bbad9ac6c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8589615d1a3448e88d22cc9b33efa0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "febe5cefd6644576a1d56cd9f609116b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_fb1c180b360e4d0bb2eb17464646462c",
            "msg_id": "",
            "outputs": []
          }
        },
        "fb1c180b360e4d0bb2eb17464646462c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}